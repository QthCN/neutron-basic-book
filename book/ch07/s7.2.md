## Docker

容器技术中，目前最有代表性的就是Docker了。本书也会以Docker为例，讲一下容器中的网络，因此我们需要熟悉一下Docker。这一节我们会讲一下Docker的基本用法、实现原理以及其代码设计层面的软件架构。


### Docker基本用法

本书不是一本关于Docker的书，所以不会给出非常详细的用法说明。对于初学者来说可以读下Docker的官方文档或者《Orchestrating Docker》一书来掌握其用法。

下面来看一个基本用法的例子，对于例子中所讲的daemon、容器、镜像等术语我们会在下一节进行说明。我们的例子是需要启动一个运行MySQL的容器。命令如下：

```
启动docker daemon：
[root@dev ~]# service docker restart
查看目前所拥有的镜像：
[root@dev ~]# docker images
REPOSITORY                       TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
libnetwork-build                 latest              c99f5cf5eb23        12 days ago         587.5 MB
docker.io/golang                 1.4                 124e2127157f        2 weeks ago         517.2 MB
docker.io/berngp/docker-zabbix   latest              ad689c775bbf        6 weeks ago         1.134 GB
docker.io/centos                 7                   7322fbe74aa5        6 weeks ago         172.2 MB
docker.io/centos                 latest              7322fbe74aa5        6 weeks ago         172.2 MB
下载一个MySQL镜像，其源托管在https://registry.hub.docker.com/_/mysql/上：
[root@dev ~]# docker pull mysql
latest: Pulling from docker.io/mysql
4c8cbfd2973e: Downloading [=>                                                 ] 1.129 MB/37.21 MB
60c52dbe9d91: Download complete 
4c8cbfd2973e: Pull complete 
60c52dbe9d91: Pull complete 
c2b0136be90f: Pull complete 
273cd71eacf0: Pull complete 
543ff72402d8: Pull complete 
aa3022270c68: Pull complete 
39130042665d: Pull complete 
2e4d19227c16: Pull complete 
1f877cc70688: Pull complete 
7e6d170eec04: Pull complete 
07264b223269: Pull complete 
b95dfc449f80: Pull complete 
45d84ed3b24d: Pull complete 
bcf7334ef42a: Pull complete 
a128139aadf2: Already exists 
docker.io/mysql:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.
Digest: sha256:3e633be4546d5549c35f9731280e7c7ef0a272c23dfbe241e08a655920c2ffa1
Status: Downloaded newer image for docker.io/mysql:latest
从上面给出的连接中可以看出这个镜像提供了一些参数可以在启动的时候使用，包括：MYSQL_ROOT_PASSWORD、MYSQL_DATABASE、MYSQL_USER、MYSQL_PASSWORD等等。这里我们使用MYSQL_DATABASE、MYSQL_USER、MYSQL_PASSWORD这三个操作来让我们的镜像启动的时候自动建立一个数据库并建立有权限操作该数据库的相关用户。同时我们映射启动后容器中的3306端口对应物理机的6033端口：
[root@dev ~]# docker run --name NeutronMySQL -p 6033:3306 -e MYSQL_USER=neutron -e MYSQL_PASSWORD=password -e MYSQL_DATABASE=neutron -e MYSQL_ROOT_PASSWORD=password docker.io/mysql
Running mysql_install_db
2015-08-02 15:06:58 0 [Note] /usr/sbin/mysqld (mysqld 5.6.26) starting as process 33 ...
2015-08-02 15:06:58 33 [Note] InnoDB: Using atomics to ref count buffer pool pages
2015-08-02 15:06:58 33 [Note] InnoDB: The InnoDB memory heap is disabled
2015-08-02 15:06:58 33 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2015-08-02 15:06:58 33 [Note] InnoDB: Memory barrier is not used
......
现在我们在物理机上连接一下我们的neutron数据库试试：
[root@dev ~]# mysql -uneutron -ppassword -P6033 -h127.0.0.1 neutron
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MySQL connection id is 2
Server version: 5.6.26 MySQL Community Server (GPL)

Copyright (c) 2000, 2014, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MySQL [neutron]> 
```
可以看到通过docker run这么一个命令我们就运行了一个完整的数据库，并且这个运行的时候可以指定某些参数的值，这一点如果要通过虚拟机来实现那么是比较麻烦的。


### Docker实现原理

上面看过一个例子后，这里我们来讲一下Docker的实现原理。Docker的实现依赖两个东西：

1. namespace
2. CGroup

namespace在前面我们讲过，主要用于提供名字空间的隔离。我们之前分析过网络namespace的实现，因此这里就不多介绍了。CGroup的作用大家可以参考这个连接的介绍：

* http://coolshell.cn/articles/17049.html

简单的说，CGroup的作用就是限制某个进程可以使用的系统资源。比如限制一个进程只可以使用1个G的内存，或者限制这个进程的IO速率等等。

当我们运行docker XXX命令的时候，大部分命令发送给了一个叫做daemon的进程。这个daemon进程的启动、停止方式也是通过docker命令完成的：

```
# 启动daemon
[root@dev ~]# docker -d
```

接下来当我们执行如docker ps这种查看当前有哪些正在运行的容器的命令的时候，docker ps会的转成一个满足RESTful要求的HTTP请求，然后这个HTTP请求会通过TCP或者本地套接字的方式发送给我们的daemon进程。daemon守护进程进行相关操作后会返回结果给我们运行docker ps的客户端，接着docker ps会的输出结果。也就是说docker其实就是在我们的本机启动了一个server，然后docker ps这类命令就是和我们的server进行普通的交互，且这种交互是基于HTTP的请求。

现在来解释下什么是容器。在上面的MySQL例子执行后，我们在系统中可以看到如下进程：

```
[root@dev ~]# ps -elf | grep NeutronMySQL
4 S root      2535  2204  0  80   0 - 51809 ep_pol 23:06 pts/0    00:00:00 docker run --name NeutronMySQL -p 6033:3306 -e MYSQL_USER=neutron -e MYSQL_PASSWORD=password -e MYSQL_DATABASE=neutron -e MYSQL_ROOT_PASSWORD=password docker.io/mysql
```

对于Docker来说，一个容器其实就是一个进程。这个进程运行在自己独立的namespace下面，所以这个pid为2535进程其看到的namespace都是独立的。当我们执行docker run命令后，docker首先会启动一个进程，然后设置这个进程使用新的namespace（如果对namespace不熟悉的话，建议去第五章复习下namespace的相关知识）。接着会在这个namespace中运行预定义的一些指令，比如启动一个MySQL进程，然后建立一个数据库等等。可以看到这个进程之所以能和我们的物理机隔离的关键正是在于namespace。那么CGroup有什么用呢？我们的例子里并没有限制我们的容器（也就是我们的进程）能使用多少系统资源，加入我希望我的MySQL容器只能使用10个G的系统内存，那么这里可以通过CGroup实现资源限制。

接着我们来解释下镜像。上面说了容器其实就是一个进程，生活在自己独立的namespace中。那么这个容器进程所看到的文件系统是什么呢？这个容器进程所看到的文件系统是DOcker通过chroot来实现的。chroot是Linux很早就提供的一个命令，用于修改某个进程所看到的根文件系统路径。比如在进程A的执行代码中执行类似chroot /tmp/a的命令，则在此之后A执行ls /命令看到的就是/tmp/a下面的内容了。因此在Docker中每个容器都是通过chroot命令获取自己独立的文件系统的。但是如果每次启动容器都需要安装MySQL等等才能使用的话会很麻烦，所以有人会事先建立好类似/tmp/a之类的目录，在这个目录下会建立如/tmp/a/bin/mysqld、/tmp/a/etc/my.cnf之类的文件并进行配置，接着容器启动的时候设置/tmp/a为根目录后，其就能执行mysqld命令，并且获取到预先设置的my.cnf配置文件了。这样的一个/tmp/a所打包生成的一个文件其实就是一个镜像的雏形。那么实际上的镜像和这个雏形有什么区别呢？如果按照我们刚刚说的，每个容器启动后都需要chroot自己的根文件系统，则如果有100个容器则会有100个类似于/tmp/a的目录存在，这样对于资源的利用以及容器的启动速度（也就是这个进程的初始化速度）是很不利的。因为类似于mysqld这样的文件大家都是公用的，完全没有必要在/tmp/a/bin下有一份，同时在/tmp/b/bin下也有一份。为了解决这个问题Docker使用了支持多层次的文件系统，比如AUFS或者Device Mapper。他们的作用简单的说就是对于容器A，启动的时候系统不会从/tmp/mysql复制一个完整的目录到/tmp/a下，而是建立一个空的/tmp/A。然后通过AUFS或Device Mapper的技术挂载出一个/tmp/a目录，对该目录的所有读操作会先在/tmp/A下进行，如果/tmp/A下没有找到对应的文件则会去/tmp/mysql下读取。对于该目录的所有写操作则都只会发生在/tmp/A。比如用户修改了my.cnf，则实际上的操作是从/tmp/mysql/etc/my.cnf复制一份到/tmp/A/etc/my.cnf，然后再对/tmp/A/etc/my.cnf进行修改。Docker在启动容器的时候，会先设置好这些，然后再chroot /tmp/a为根目录。可以看到通过这种多层次的文件系统用户可以对底层的/tmp/mysql进行定制后再提供服务给其他人使用，比如用户在进行上面的操作后，安装了一个Apache+PHP在容器中，此时我们的/tmp/mysql + /tmp/A合并而成的/tmp/a就能提供一个完整的LAMP服务了。于是可以将/tmp/mysql + /tmp/A一起打包成一个文件提供出去。当某个用户需要一个LAMP的容器B在上面运行一个wordpress应用的时候，其解压这个文件，然后Docker建立空的/tmp/B，并通过上面说的AUFS或Device Mapper将/tmp/mysql、/tmp/A、/tmp/B挂载为/tmp/b，然后启动的新容器chroot到/tmp/b后就能直接使用LAMP环境了。这里说到的/tmp/mysql加上/tmp/A以及一些元信息所共同打包二层的一个文件就是我们所说的镜像。可以看到镜像是分层的。

以上这些就是Docker的实现原理，说实话从原理上看真的很简单，因为Docker使用的技术都是一些现有的技术。


### Docker代码实现导读

由于读者在看完本章后可能会去深入学习libnetwork的代码，所以笔者这里简单的说下Docker的代码如何去阅读，帮助大家快速的定位libnetwork在Docker代码中的地位及使用场合。

再讲Docker代码之前，先给大家说下GO语言运行或编译时候的一些机制，笔者在学习Docker的时候就因为GO的基本功不扎实所以看漏了很多东西。对于GO不熟悉的读者可以跳过这一部分。

1. packege的init函数会的在main函数之前运行。Docker在代码中实现了很多的钩子函数，很多模块会的注册自己的实现到这些钩子上。这些注册的时机一般都发生在package的init函数中。所以看代码的时候如果发现有个地方调用预先注册的钩子，那么在查看实现的时候就需要去init函数中看下是不是在init中注册的钩子。
2. "./..."。GO中的...具有特殊的含义，表示嵌套递归所有的子目录及子目录下的子目录中的文件。大家在看Makefile的时候如果看到...就知道这个命令是作用于该文件夹下面的所有文件的。
3. "+build"。在一些文件的最开头可以看到类似"// +build libnetwork_discovery"这样的代码，这类代码大家理解为影响编译器的宏即可。比如"go build -tags libnetwork_discovery ./..."则会在发现相同实现的时候，选择拥有"// +build libnetwork_discovery"的代码进行编译。

我们来看下Docker的代码。看代码要从Makefile开始看起，这里Makefile笔者就不多说了，Docker的代码有个比较好玩的地方在于其编译、连接及测试都是跑在一个容器中的，Makefile中给出了其实现。

对于docker来说，入口函数为main，main中笔者想分析的代码为：

```
if *flDaemon {
    if *flHelp {
        flag.Usage()
        return
    }
    mainDaemon()
    return
}
 
// From here on, we assume we're a client, not a server.
```

如果docker命令启动的时候加了-D，则这里的flDaemon这个flag就是True，于是会执行mainDaemon()启动我们的docker server。我们先分析client的。一个client在docker中为：

```
cli := client.NewDockerCli(stdin, stdout, stderr, *flTrustKey, protoAddrParts[0], protoAddrParts[1], tlsConfig)
 
if err := cli.Cmd(flag.Args()...); err != nil {
    if sterr, ok := err.(client.StatusError); ok {
        if sterr.Status != "" {
            fmt.Fprintln(cli.Err(), sterr.Status)
            os.Exit(1)
        }
        os.Exit(sterr.StatusCode)
    }
    fmt.Fprintln(cli.Err(), err)
    os.Exit(1)
}

......

// The transport is created here for reuse during the client session.
tr := &http.Transport{
    TLSClientConfig: tlsConfig,
}
sockets.ConfigureTCPTransport(tr, proto, addr)
 
configFile, e := cliconfig.Load(cliconfig.ConfigDir())
if e != nil {
    fmt.Fprintf(err, "WARNING: Error loading config file:%v\n", e)
}
 
return &DockerCli{
    proto:         proto,
    addr:          addr,
    configFile:    configFile,
    in:            in,
    out:           out,
    err:           err,
    keyFile:       keyFile,
    inFd:          inFd,
    outFd:         outFd,
    isTerminalIn:  isTerminalIn,
    isTerminalOut: isTerminalOut,
    tlsConfig:     tlsConfig,
    scheme:        scheme,
    transport:     tr,
}
```

当docker执行client的命令的时候，会通过GO提供的HTTP Client发送请求给docker server。什么时候调用HTTP Client我们下面会看到，先来看下cli.Cmd的实现：

```
// Cmd executes the specified command.
func (cli *DockerCli) Cmd(args ...string) error {
    if len(args) > 1 {
        method, exists := cli.getMethod(args[:2]...)
        if exists {
            return method(args[2:]...)
        }
    }
    if len(args) > 0 {
        method, exists := cli.getMethod(args[0])
        if !exists {
            return fmt.Errorf("docker: '%s' is not a docker command.\nSee 'docker --help'.", args[0])
        }
        return method(args[1:]...)
    }
    return cli.CmdHelp()
}
......
func (cli *DockerCli) getMethod(args ...string) (func(...string) error, bool) {
    camelArgs := make([]string, len(args))
    for i, s := range args {
        if len(s) == 0 {
            return nil, false
        }
        camelArgs[i] = strings.ToUpper(s[:1]) + strings.ToLower(s[1:])
    }
    methodName := "Cmd" + strings.Join(camelArgs, "")
    method := reflect.ValueOf(cli).MethodByName(methodName)
    if !method.IsValid() {
        return nil, false
    }
    return method.Interface().(func(...string) error), true
}

```

这里通过反射的方法获取了实际的命令。类似于Python中的getattr方法。在client的目录下我们可以看到这些Cmd打头的函数：

```
Cmd
CmdAttach
CmdBuild
CmdCommit
CmdCp
CmdCreate
CmdDiff
CmdEvents
CmdExec
CmdExport
CmdHelp
CmdHistory
CmdImages
CmdImport
CmdInfo
CmdInspect
CmdKill
CmdLoad
CmdLogin
CmdLogout
CmdLogs
CmdNetwork
CmdPause
CmdPort
CmdPs
CmdPull
CmdPush
CmdRename
CmdRestart
CmdRm
CmdRmi
CmdRun
CmdSave
CmdSearch
CmdService
CmdStart
CmdStats
CmdStop
CmdTag
CmdTop
CmdUnpause
CmdVersion
CmdWait
```

因此我们要分析一个Client请求如何实现的时候我们只要这里选取一个分析即可。我们来看下CmdPs的实现，其重要的代码为：

```
rdr, _, _, err := cli.call("GET", "/containers/json?"+v.Encode(), nil, nil)
if err != nil {
    return err
}
 
defer rdr.Close()
 
containers := []types.Container{}
if err := json.NewDecoder(rdr).Decode(&containers); err != nil {
    return err
}
 
w := tabwriter.NewWriter(cli.out, 20, 1, 3, ' ', 0)
if !*quiet {
    fmt.Fprint(w, "CONTAINER ID\tIMAGE\tCOMMAND\tCREATED\tSTATUS\tPORTS\tNAMES")
 
    if *size {
        fmt.Fprintln(w, "\tSIZE")
    } else {
        fmt.Fprint(w, "\n")
    }
}
```

可以看到docker将我们的请求转为一个对HTTP URL的请求。call方法是怎么实现的呢？关联代码如下：

```
func (cli *DockerCli) call(method, path string, data interface{}, headers map[string][]string) (io.ReadCloser, http.Header, int, error) {
    params, err := cli.encodeData(data)
    if err != nil {
        return nil, nil, -1, err
    }  
 
    if data != nil {
        if headers == nil {
            headers = make(map[string][]string)
        }
        headers["Content-Type"] = []string{"application/json"}
    }  
 
    serverResp, err := cli.clientRequest(method, path, params, headers)
    return serverResp.body, serverResp.header, serverResp.statusCode, err
}
 
......
 
func (cli *DockerCli) clientRequest(method, path string, in io.Reader, headers map[string][]string) (*serverResponse, error) {
 
    serverResp := &serverResponse{
        body:       nil,
        statusCode: -1,
    }
    ......
    req.Header.Set("User-Agent", "Docker-Client/"+dockerversion.VERSION+" ("+runtime.GOOS+")")
    req.URL.Host = cli.addr
    req.URL.Scheme = cli.scheme
    ......
    if expectedPayload && req.Header.Get("Content-Type") == "" {
        req.Header.Set("Content-Type", "text/plain")
    }
 
    resp, err := cli.HTTPClient().Do(req)
    ......
 
......
 
// HTTPClient creates a new HTTP client with the cli's client transport instance.
func (cli *DockerCli) HTTPClient() *http.Client {
    return &http.Client{Transport: cli.transport}
```

可以看到具体的请求最后会通过http这个packege的Client来实现。

Client端的代码就是这么一个框架，其他的Cmd命令基本上也是这么一个模式。我们接下来看下docker server端的代码。按照上面分析的，我们从mainDaemon开始看，重要的代码为：

```
serverConfig := &apiserver.ServerConfig{
    Logging:     true,
    EnableCors:  daemonCfg.EnableCors,
    CorsHeaders: daemonCfg.CorsHeaders,
    Version:     dockerversion.VERSION,
}
serverConfig = setPlatformServerConfig(serverConfig, daemonCfg)
......
api := apiserver.New(serverConfig)

......

registryService := registry.NewService(registryCfg)
d, err := daemon.NewDaemon(daemonCfg, registryService)
if err != nil {
    if pfile != nil {
        if err := pfile.Remove(); err != nil {
            logrus.Error(err)
        }
    }
    logrus.Fatalf("Error starting daemon: %v", err)
}

......

// after the daemon is done setting up we can tell the api to start
// accepting connections with specified daemon
api.AcceptConnections(d)
```

这里启动了一个apiserver。我们可以认为这个apiserver就是提供HTTP服务的server。同时这里还建立了一个daemon。这里来说下apiserver和daemon的关系。其实这个关系很简单，就是一个MVC。apiserver负责监听HTTP请求，然后分析这个请求要交由那个后端进行处理（一般我们把这个后端称为controller，而apiserver则称为view）。后端在实现代码的时候可能会需要操作AUFS或者Device Mapper，但是我们知道AUFS是Ubuntu支持的，而Ubuntu默认不支持Device Mapper，所以后端如何区别这种事情呢？后端当然不想管啦（因为要降低耦合），所以后端会调用daemon来操作。daemon会暴露出公共的接口给后端使用，后端调用daemon的接口，然后daemon的接口再去调用真正的driver来干活。这里的driver也是在启动的时候根据配置即参数初始化完成的。所以对于daemon来说笔者更加倾向于叫他model。

我们先来看下apiserver的初始化。切入口为api := apiserver.New(serverConfig)，核心代码：

```
func New(cfg *ServerConfig) *Server {
    srv := &Server{
        cfg:   cfg,
        start: make(chan struct{}),
    }   
    r := createRouter(srv)
    srv.router = r
    return srv
}
```

熟悉OpenStack中使用频率特别高的routes组件的应该能猜到，这里的router就是决定了一个URL最终映射到什么处理函数的一个路由。我们可以看到createRoute的代码：

```
// we keep enableCors just for legacy usage, need to be removed in the future
func createRouter(s *Server) *mux.Router {
    r := mux.NewRouter()
    if os.Getenv("DEBUG") != "" {
        ProfilerSetup(r, "/debug/")
    }
    m := map[string]map[string]HttpApiFunc{
        "GET": {
            "/_ping":                          s.ping,
            ......
        },
        "POST": {
            "/auth":                         s.postAuth,
            ......
        },
        "DELETE": {
            "/containers/{name:.*}": s.deleteContainers,
            ......
        },
        "OPTIONS": {
            "": s.optionsHandler,
        },
    }
 
    // If "api-cors-header" is not given, but "api-enable-cors" is true, we set cors to "*"
    // otherwise, all head values will be passed to HTTP handler
    corsHeaders := s.cfg.CorsHeaders
    if corsHeaders == "" && s.cfg.EnableCors {
        corsHeaders = "*"
    }
 
    for method, routes := range m {
        for route, fct := range routes {
            logrus.Debugf("Registering %s, %s", method, route)
            // NOTE: scope issue, make sure the variables are local and won't be changed
            localRoute := route
            localFct := fct
            localMethod := method
 
            // build the handler function
            f := makeHttpHandler(s.cfg.Logging, localMethod, localRoute, localFct, corsHeaders, version.Version(s.cfg.Version))
 
            // add the new route
            if localRoute == "" {
                r.Methods(localMethod).HandlerFunc(f)
            } else {
                r.Path("/v{version:[0-9.]+}" + localRoute).Methods(localMethod).HandlerFunc(f)
                r.Path(localRoute).Methods(localMethod).HandlerFunc(f)
            }
        }
    }
 
    return r
}
```

联系下本书之前对于routes的介绍，这里几乎就是一样的。只不过由于GO本身的问题，这里必须明确写明URL与后端处理函数的对应关系。比如"/containers/{name:.*}"这个URL必须明确指出是通过s.deleteContainers来实现其后端。

在这些URL关系都初始化后，最主要的代码就是监听HTTP请求了。这部分代码不影响我们学习libnetwork，有兴趣的读者可以自己去分析下，并不是很复杂。

在讲daemon之前我们先从代码上验证下笔者刚刚说的MVC模型。比如对于docker ps，根据上面的代码可以找到其实现为s.getContainersJSON，而后者的实现为：

```
func (s *Server) getContainersJSON(version version.Version, w http.ResponseWriter, r *http.Request, vars map[string]string) error {
    if err := parseForm(r); err != nil {
        return err
    }  
         
    config := &daemon.ContainersConfig{
        All:     boolValue(r, "all"),
        Size:    boolValue(r, "size"),
        Since:   r.Form.Get("since"),
        Before:  r.Form.Get("before"),
        Filters: r.Form.Get("filters"),     
    }      
             
    if tmpLimit := r.Form.Get("limit"); tmpLimit != "" {
        limit, err := strconv.Atoi(tmpLimit)
        if err != nil {
            return err
        }
        config.Limit = limit
    }
 
    containers, err := s.daemon.Containers(config)
    if err != nil {
        return err
    }
 
    return writeJSON(w, http.StatusOK, containers)
}
```

这里的关键在于s.daemon.Containers(config)这行代码，可以看到我们的后端函数s.getContainersJSON在得到获取容器列表的请求后，通过daemon查询了容器信息（这个例子比较巧，很容易能将daemon类比为数据库）。

现在我们看下一个请求的处理过程：用户敲下docker ps命令，docker将这个命令转成HTTP请求发给docker server，docker server得到这个HTTP请求后解析出对应的后端处理函数s.getContainersJSON，然后后端处理函数在调用daemon提供的接口获取或操作对应的对象。daemon屏蔽了HTTP这一层与底层实现的耦合。而daemon在启动的时候必然要初始化底层实现的各种driver，比如操作cgroup的driver，又比如是使用AUFS还是Device Mapper等等，这些事情都由daemon来完成。比如daemon.FSDriver在初始化的时候赋值了AUFSDriver，则上层在调用daemon.FSDriver.Get()的时候，daemon会使用AUFSDriver.GET()来完成请求。这就是一个请求的基本处理过程。

在上面说了daemon的作用后，笔者这里特意列下其初始化代码中的几个重要组件：

```
......
driver, err := graphdriver.New(config.Root, config.GraphOptions)
......
d.netController, err = initNetworkController(config)
......
ed, err := execdrivers.NewDriver(config.ExecDriver, config.ExecOptions, config.ExecRoot, config.Root, sysInitPath, sysInfo)
```

这里的graphdriver.New会根据配置文件决定是使用AUFS还是Device Mapper还是其他什么Driver。initNetworkController会初始化网络环境，并决定使用何种网络的Driver，execdrivers.NewDriver会初始化执行环境，并决定使用何种执行Driver。这些对象初始化后都会赋值给daemon。

现在的趋势是网络Driver使用libnetwork，而执行Driver使用libcontainer。libnetwork是一个GO实现的网络相关操作的package，而libnetwork则是GO实现的一个操作CGroup等资源package。这两个package都可以给其它项目使用。我们来看下initNetworkController的实现：

```
func initNetworkController(config *Config) (libnetwork.NetworkController, error) {
    netOptions, err := networkOptions(config)
    if err != nil {
        return nil, err
    }
 
    controller, err := libnetwork.New(netOptions...)
    if err != nil {
        return nil, fmt.Errorf("error obtaining controller instance: %v", err)
    }
```
可以看到这里是通过调用libnetwork的相关函数来实现网络Controller的init的。这里我们第一次看到了libnetwork，并且也对其在Docker中何时会被调用有了一个基本认识。可以想象在docker的那些网络相关的命令中，最后后端代码的实现肯定是调用daemon的相关函数，而这些daemon的相关函数的实现肯定是通过libnetwork实现的。这里我们把这条路给串起来了。


### Docker中网络相关的规则

本节最后我们来看一些Docker中网络建立后的iptables及路由表规则。

在docker daemon没有启动的时候，可以发现系统上只有libvirt建立的默认网桥：
```
[root@dev ~]# systemctl status  docker.service
docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled)
   Active: inactive (dead)
     Docs: http://docs.docker.com

[root@dev ~]# brctl show
bridge name bridge id       STP enabled interfaces
virbr0      8000.52540027e8bc   yes     virbr0-nic
```

同时观察对应的iptables规则，可以看到规则是很干净的：
```
[root@dev ~]# iptables-save 
# Generated by iptables-save v1.4.21 on Thu Aug  6 13:40:20 2015
*nat
:PREROUTING ACCEPT [13:997]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
COMMIT
# Completed on Thu Aug  6 13:40:20 2015
# Generated by iptables-save v1.4.21 on Thu Aug  6 13:40:20 2015
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [44:5440]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
# Completed on Thu Aug  6 13:40:20 2015
```

现在启动docker daemon：
```
[root@dev ~]# service docker start
Redirecting to /bin/systemctl start  docker.service
[root@dev ~]# brctl show
bridge name bridge id       STP enabled interfaces
docker0     8000.56847afe9799   no      
virbr0      8000.52540027e8bc   yes     virbr0-nic
[root@dev ~]# ip l show dev docker0
8: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT 
    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff
```

可以发现出现了一个docker0的bridge。同时我们看下iptables规则：
```
[root@dev ~]# iptables-save 
# Generated by iptables-save v1.4.21 on Thu Aug  6 13:41:52 2015
*nat
:PREROUTING ACCEPT [22:1779]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
COMMIT
# Completed on Thu Aug  6 13:41:52 2015
# Generated by iptables-save v1.4.21 on Thu Aug  6 13:41:52 2015
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [18:2944]
:DOCKER - [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
# Completed on Thu Aug  6 13:41:52 2015
```

可以看到在还没启动任何容器的情况下，nat和filter这两个表中都添加了规则，我们来看下这些规则。首先我们再来复习下iptables的一些基本知识，当网卡收到一个数据包后，内核会决定这个数据包是发送给本地的程序，还是转发给其他的主机。如果这个数据包是发给本机的，则会的走：mangle-PREROUTING -> nat-PREROUTING -> mangle-INPUT -> filter-INPUT这个顺序，然后走到接收的应用。如果一个数据包是从本机发送出去的，那么会走：mangle-OUTPUT -> nat-OUTPUT -> filter-OUTPUT -> mangle-POSTROUTING -> nat-POSTROUTING这个顺序。如果一个数据包收到后是要转发出去的，则会走mangle-PREROUTING -> nat-PREROUTING -> mangle-FORWARD -> filter-FORWARD -> mangle-POSTROUTING -> nat-POSTROUTING这个顺序。有了这个基础后我们看下上面docker的iptables规则。也来看三种情况：

1. 数据包发送到本机。根据我们上面的顺序，会的走mangle-PREROUTING -> nat-PREROUTING -> mangle-INPUT -> filter-INPUT。从nat表中可以看到-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER，也就是说这里通过addrtype这个module来匹配数据包，如果这个数据包是发给本地的那么走DOCKER的链。目前我们的DOCKER链是空的。
2. 数据包发送出去。根据我们上面的顺序，会的走mangle-OUTPUT -> nat-OUTPUT -> filter-OUTPUT -> mangle-POSTROUTING -> nat-POSTROUTING。这里我们看到两条规则-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER以及-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE。第一条规则表明如果这个数据包的目的地址是非127的LOCAL地址则走DOCKER，第二天是个SNAT，表示非docker0这个口发送的源地址网段为172.17.0.0/16的包在出去的时候会走NAT。这个规则应该保证了我们docker的容器可以访问外网。
3. 数据包的转发。根据我们上面的顺序，会的走mangle-PREROUTING -> nat-PREROUTING -> mangle-FORWARD -> filter-FORWARD -> mangle-POSTROUTING -> nat-POSTROUTING这个顺序。这里有四条规则是和DOCKER相关的，最中要的作用是说如果一个数据包的目的设备是docker0，则走DOCKER链。

现在我们启动一个容器，不指定任何的网络参数：

```
[root@dev ~]# docker run -dit --name test-os docker.io/centos /bin/bash
[/code]
可以看到docker0这个bridge中被plug了一个veth：
[code lang="bash"]
[root@dev ~]# brctl show
bridge name bridge id       STP enabled interfaces
docker0     8000.56847afe9799   no      vethe7cd2e3
virbr0      8000.52540027e8bc   yes     virbr0-nic
[root@dev ~]# ip l show dev vethe7cd2e3
16: vethe7cd2e3: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT 
    link/ether f6:b9:36:fb:5b:ac brd ff:ff:ff:ff:ff:ff
[root@dev ~]# ip a show dev vethe7cd2e3
16: vethe7cd2e3: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP 
    link/ether f6:b9:36:fb:5b:ac brd ff:ff:ff:ff:ff:ff
    inet6 fe80::f4b9:36ff:fefb:5bac/64 scope link 
       valid_lft forever preferred_lft forever
```

而veth的另一头则在我们的test-os的namespace中。需要注意的是docker对namespace的操作是直接通过套接字发送到内核的，没有像Neutron一样使用ip命令。根据之前的文章我们知道ip命令能查看到的namespace必须是ip自己建立的，因此这里不能直接通过ip命令查看namespace。

我们看下iptables此时的规则变化：

```
[root@dev ~]# iptables-save
# Generated by iptables-save v1.4.21 on Thu Aug  6 14:12:58 2015
*nat
:PREROUTING ACCEPT [5481:502017]
:INPUT ACCEPT [4:256]
:OUTPUT ACCEPT [57:4519]
:POSTROUTING ACCEPT [57:4519]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
COMMIT
# Completed on Thu Aug  6 14:12:58 2015
# Generated by iptables-save v1.4.21 on Thu Aug  6 14:12:58 2015
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [1375:163760]
:DOCKER - [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrac`k --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
# Completed on Thu Aug  6 14:12:58 2015
```

可以看到iptables的规则没有发生变化。此时如果容器要发送请求给公网，那么这个请求会通过veth走到物理机namespace的vethe7cd2e3。由于这个vethe7cd2e3是plug到docker0上的，所以会的走网桥的逻辑。由于我们的内核设置了允许ip forward，所以这个包会从docker0出来传递到内核。对于内核来说这是一次forward的请求，所以会的走转发链，也就是走mangle-PREROUTING -> nat-PREROUTING -> mangle-FORWARD -> filter-FORWARD -> mangle-POSTROUTING -> nat-POSTROUTING。如果在容器中ping 1.2.4.8，iptables的统计信息中可以看到：

```
[root@dev ~]# iptables -t nat -L -xvn
Chain PREROUTING (policy ACCEPT 6518 packets, 593756 bytes)
    pkts      bytes target     prot opt in     out     source               destination         
       5      316 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT 4 packets, 256 bytes)
    pkts      bytes target     prot opt in     out     source               destination         

Chain OUTPUT (policy ACCEPT 58 packets, 4574 bytes)
    pkts      bytes target     prot opt in     out     source               destination         
       0        0 DOCKER     all  --  *      *       0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT 58 packets, 4574 bytes)
    pkts      bytes target     prot opt in     out     source               destination         
       2      168 MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0           

Chain DOCKER (2 references)
    pkts      bytes target     prot opt in     out     source               destination  
```

这里POSTROUTING的pkts可以看到增加。之所以这里的pkts数目和实际上的ping包个数不一致是由于conntrackd的session信息有一定的默认cache时间造成的。

再来看下物理机想要访问容器的数据链路。当我们访问容器的172.17.0.5这个地址的时候，根据路由这个数据包是走到docker0的：

```
[root@dev ~]# ip r
default via 172.16.1.1 dev enp0s3  proto static  metric 100 
default via 10.0.2.1 dev enp0s8  proto static  metric 101 
10.0.2.0/24 dev enp0s8  proto kernel  scope link  src 10.0.2.6 
10.0.2.0/24 dev enp0s8  proto kernel  scope link  src 10.0.2.6  metric 100 
172.16.1.0/24 dev enp0s3  proto kernel  scope link  src 172.16.1.75  metric 100 
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.42.1 
192.168.56.0/24 dev enp0s9  proto kernel  scope link  src 192.168.56.200  metric 100 
192.168.100.0/24 dev enp0s10  proto kernel  scope link  src 192.168.100.101  metric 100 
192.168.122.0/24 dev virbr0  proto kernel  scope link  src 192.168.122.1 
[/code]
当数据包走到docker0后走正常的bridge的逻辑，就能到ethe7cd2e3，然后到达对端的namespace中的veth，被我们的容器接收。对于容器与容器之间的互相访问也和这里类似。

现在再来看一个使用了网络参数的例子。比如我们希望映射物理机的8888到容器的80端口，我们来看下在这种情况下的规则变化：
[code lang="bash"]
[root@dev ~]# docker run -dit -p 8888:80 --name test-os2 docker.io/centos /bin/bash
```

此时看下iptables：

```
[root@dev ~]# iptables-save 
# Generated by iptables-save v1.4.21 on Thu Aug  6 14:29:18 2015
*mangle
:PREROUTING ACCEPT [2644:222950]
:INPUT ACCEPT [2588:218246]
:FORWARD ACCEPT [56:4704]
:OUTPUT ACCEPT [393:75016]
:POSTROUTING ACCEPT [449:79720]
COMMIT
# Completed on Thu Aug  6 14:29:18 2015
# Generated by iptables-save v1.4.21 on Thu Aug  6 14:29:18 2015
*nat
:PREROUTING ACCEPT [38:2854]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [1:55]
:POSTROUTING ACCEPT [1:55]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.6/32 -d 172.17.0.6/32 -p tcp -m tcp --dport 80 -j MASQUERADE
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 8888 -j DNAT --to-destination 172.17.0.6:80
COMMIT
# Completed on Thu Aug  6 14:29:18 2015
# Generated by iptables-save v1.4.21 on Thu Aug  6 14:29:18 2015
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [29:2822]
:DOCKER - [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
-A DOCKER -d 172.17.0.6/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT
COMMIT
# Completed on Thu Aug  6 14:29:18 2015
```

可以看到这里多了很多和80、8888相关的规则，我们来分析下。首先来看一个数据包需要从容器发送到公网的情况，此时会走转发的逻辑，也就是我们上面说的mangle-PREROUTING -> nat-PREROUTING -> mangle-FORWARD -> filter-FORWARD -> mangle-POSTROUTING -> nat-POSTROUTING。此时数据包已经经过网桥的逻辑，从docker0进入到内核，然后走nat-PREROUTING，也就是会的走DOCKER的链，但DOCKER链! -i docker0这个要求使得这个数据包没有被匹配上，于是就继续走剩余的规则。filter中的FORWARD也没有匹配的，于是最后走nat-POSTROUTING，也就是通过SNAT出去了。现在来看下公网访问物理机的8888的流程，当这个数据包进入内核后，内核走nat-PREROUTING，然后匹配上了-A DOCKER ! -i docker0 -p tcp -m tcp --dport 8888 -j DNAT --to-destination 172.17.0.6:80，接着内核将这个数据包的目的地址改为172.17.0.6:80，接着就发送给了我们的容器。


关于Docker的笔者就简单介绍这么多。有兴趣的读者可以自行去学习。接下来我们会讲libnetwork的实现以及和Neutron相关的结合，这才是我们关注的重点。

