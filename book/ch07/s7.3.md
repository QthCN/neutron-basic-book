## Kubernetes

在本章的一开始笔者提到基于容器提供一个PaaS是一个比较合适的方案，实际上目前一些大型厂商的PaaS服务就是基于类似容器的技术来提供的，而不是叠加在IaaS对外提供。当然笔者这里不讨论这样做是否合适。如果要基于容器提供PaaS服务那么必然要有一个调度控制器来管理我们的容器。目前Google的Kubernetes就是一个这样的平台。这里我们来讲一下这个Kubernetes。

Kubernetes笔者个人倾向于其是一个任务调度系统。比如一个WEB应用运行在Tomcat上，现在这个应用要上线了，在Kubernetes中的做法就是将这个WEB应用及Tomcat等先做成一个容器，然后在Kubernetes里写好这个应用的一些描述。接着通过Kubernetes的指令在一个集群中启动这个应用。使用者只要关心在容器中启动应用即可，Kubernetes会的根据描述信息自动的在集群中找到合适的物理机，然后在上面下载容器镜像并启动容器。一般为了高可用我们可以在描述信息中描述好副本个数，比如我们指定这个应用可以存在三个副本，则我们在启动应用的时候Kubernetes会的在集群中启动三个相同的应用，同时Kubernetes中存在一个Service的概念，对这个概念最简单的理解就是可以把它当成一个负载均衡器，外界来访问我们的WEB应用的时候看到的是Service的地址。当集群中运行应用容器的主机异常或宕机的时候，Service会的帮我们屏蔽这种异常。同时Kubernetes会根据副本个数保证集群中始终维持要求的副本个数。

在Kubernetes中，上面讲到的应用在其术语中称为Pod。大家可以认为一个Pod就是一个独立、完全的集合体，比如我们的应用是MySQL+Apache+PHP，那么在一个Pod中可以运行一个MySQL的容器以及一个Apache+PHP的容器，然后对外提供服务的时候一个Pod就够了，不需要额外的其它容器了。

对于Kubernetes的网络是比较值得研究的一个话题。Docker由于目前局限在单物理机的状态，所以其网络并不是非常复杂。但对于Kubernetes来说由于是一个集群，因此Pod与Pod之间、Pod与外界之间的网络就必须要做到足够的强大才能满足业务的支撑。在Kubernetes中一个Pod拥有一个独立的IP，在这个Pod中的所有容器都直接使用这个IP而不需要走NAT，因此Pod中的容器看到的自身IP就是外界看到的这个容器的IP。这样做的好处是对以前运行在虚拟机环境的业务能非常容易的进行迁移。比如我们上面的MySQL+Apache+PHP的例子，在传统虚拟化里一般是启动一台虚拟机然后上面分别运行MySQL以及Apache进程，这些进程监听的IP就是虚拟机的IP。此时将这一套迁移到Pod的时候网络方面就简单很多。当Pod需要和外界进行交互的时候Kubernetes目前有很多方案，按照刚刚说的例子这里大家可以把Pod想成我们的Nova虚拟机，这样大家就可以用Neutron的思想来套Kubernetes的网络方案了。比如复杂一点的，Pod中的数据流出来后接一个OVS，然后OVS打上vxlan头发送出去。

### 基于OVS的多节点Kubernetes搭建过程

这里给出一个基于OVS的多节点Kubernetes的搭建过程，官方这部分文档目前还比较缺。一共三台主机，k8s01跑相关的控制器，k8s02和k8s03用来跑容器。首先我们先建立三台虚拟机，系统使用Fedora 22。主机信息如下：

```
192.168.0.33 k8s01
192.168.0.34 k8s02
192.168.0.35 k8s03
```

下面是具体的部署步骤。

首先，在分别在三台主机的/etc/hosts文件中配置好相关的主机名与IP信息：
```
[root@k8s01 ~]# echo "192.168.0.33 k8s01
> 192.168.0.34 k8s02
> 192.168.0.35 k8s03" >> /etc/hosts
```

接着在三台主机上安装相关的软件包：
```
[root@k8s01 ~]# yum -y install --enablerepo=updates-testing kubernetes
[root@k8s01 ~]# yum -y install etcd iptables
```

三台主机上都把防火墙停了：
```
[root@k8s01 ~]# systemctl disable iptables-services firewalld
[root@k8s01 ~]# systemctl stop iptables-services firewalld
Failed to stop iptables-services.service: Unit iptables-services.service not loaded.
```

k8s02和k8s03上开启转发功能：
```
[root@k8s02 ~]# echo "1" > /proc/sys/net/ipv4/ip_forward
```

接着我们修改下k8s的配置，首先三台主机的/etc/kubernetes/config都改为如下配置：
```
[root@k8s01 ~]# cat /etc/kubernetes/config  | grep -v '#'
KUBE_LOGTOSTDERR="--logtostderr=true"

KUBE_LOG_LEVEL="--v=0"

KUBE_ALLOW_PRIV="--allow_privileged=false"

KUBE_MASTER="--master=http://k8s01:8080"
```

生成一个key：
```
[root@k8s01 ~]# openssl genrsa -out /tmp/serviceaccount.key 2048
Generating RSA private key, 2048 bit long modulus
........................................................................................+++
.........+++
e is 65537 (0x10001)
```

接着在k8s01上修改/etc/kubernetes/apiserver如下：
```
[root@k8s01 ~]# cat /etc/kubernetes/apiserver | grep -vE '(#|^$)'
KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"
KUBE_ETCD_SERVERS="--etcd_servers=http://127.0.0.1:4001"
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"
KUBE_ADMISSION_CONTROL="--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"
KUBE_API_ARGS="--service_account_key_file=/tmp/serviceaccount.key"
```

在k8s01上修改/etc/kubernetes/controller-manager如下：
```
[root@k8s01 ~]# cat /etc/kubernetes/controller-manager  | grep -vE '(#|^$)'
KUBE_CONTROLLER_MANAGER_ARGS="--service_account_private_key_file=/tmp/serviceaccount.key"
```

在k8s01上修改etcd的监听端口，将/etc/etcd/etcd.conf中的ETCD_LISTEN_CLIENT_URLS改为"http://0.0.0.0:4001"：
```
[root@k8s01 ~]# cat /etc/etcd/etcd.conf | grep ETCD_LISTEN_CLIENT_URLS
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:4001"
```

在k8s01上启动对应的控制服务：
```
[root@k8s01 ~]# for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler; do
>     systemctl restart $SERVICES
>     systemctl enable $SERVICES
>     systemctl status $SERVICES
> done
```

在k8s02和k8s03上修改/etc/kubernetes/kubelet文件，内容如下：
```
[root@k8s02 ~]# cat /etc/kubernetes/kubelet  | grep -vE '(#|^$)'
KUBELET_ADDRESS="--address=0.0.0.0"
KUBELET_HOSTNAME="--hostname_override=k8s02"
KUBELET_API_SERVER="--api_servers=http://k8s01:8080"
KUBELET_ARGS=""
```

在k8s02及k8s03上启动相关服务：
```
[root@k8s02 ~]# for SERVICES in kube-proxy kubelet docker; do 
>     systemctl restart $SERVICES
>     systemctl enable $SERVICES
>     systemctl status $SERVICES 
> done
```

现在我们来设置网络环境。k8s02和k8s03的两个docker0会通过veth接到各自的ovs的bridge上，然后这两个bridge通过vxlan打通。下面具体的设置步骤：
首先先在k8s02和k8s03上安装软件包：
```
[root@k8s02 ~]# yum install -y bridge-utils openvswitch 
```

需要改变下docker0的默认网段，目前在k8s02以及k8s03上docker0都是使用了172.17.42.1/16。我们改成k8s02使用172.17.20.1/24，k8s03使用172.17.30.1/24。下面是具体步骤：
```
# k8s02
[root@k8s02 ~]# systemctl stop docker.service
[root@k8s02 ~]# ip l set dev docker0 down
[root@k8s02 ~]# brctl delbr docker0
[root@k8s02 ~]# nohup docker -d --bip=172.17.20.1/24 --fixed-cidr=172.17.20.0/24 &
[root@k8s03 ~]# ip r add 172.17.30.0/24 dev docker0

# k8s03
[root@k8s03 ~]# systemctl stop docker.service
[root@k8s03 ~]# ip l set dev docker0 down
[root@k8s03 ~]# brctl delbr docker0
[root@k8s03 ~]# nohup docker -d --bip=172.17.30.1/24 --fixed-cidr=172.17.30.0/24 &
[root@k8s02 ~]# ip r add 172.17.20.0/24 dev docker0
```

在k8s02和k8s03上启动ovs服务：
```
[root@k8s02 ~]# systemctl start openvswitch.service
```

在k8s02和k8s03上各自建立一个ovs的bridge br-tun：
```
[root@k8s02 ~]# ovs-vsctl add-br br-tun
```

在k8s02和k8s03上建立veth对，连接br-tun和docker0：
```
[root@k8s02 ~]# ip link add veth-docker type veth peer name veth-ovs
[root@k8s02 ~]# ovs-vsctl add-port br-tun veth-ovs
[root@k8s02 ~]# brctl addif docker0 veth-docker
[root@k8s02 ~]# ip l set dev veth-ovs up
[root@k8s02 ~]# ip l set dev veth-docker up
```

接着我们配置下k8s02和k8s03之间的vxlan隧道，k8s02上进行如下配置：
```
[root@k8s02 ~]# ovs-vsctl add-port br-tun port-vxlan -- set Interface port-vxlan type=vxlan options:remote_ip=192.168.0.35
```

k8s03上进行如下配置：
```
[root@k8s03 ~]# ovs-vsctl add-port br-tun port-vxlan -- set Interface port-vxlan type=vxlan options:remote_ip=192.168.0.34
```

现在我们来测试下vxlan隧道是否起作用，我们在k8s03上建立一个veth对，然后将veth的一头放到docker0上，另一头我们配置一个ip。接着我们尝试在k8s02上ping这个地址，如果一切正常的话应该是可以ping通的：
```
# k8s03上的操作
[root@k8s03 ~]# ip link add debug-docker type veth peer name debug-host
[root@k8s03 ~]# brctl addif docker0 debug-docker
[root@k8s03 ~]# ip l set dev debug-docker up
[root@k8s03 ~]# ip l set dev debug-host up
[root@k8s03 ~]# ip a change 172.17.30.99 dev debug-host

# k8s02上的操作
[root@k8s02 ~]# ping 172.17.30.99 -c 4
PING 172.17.30.99 (172.17.30.99) 56(84) bytes of data.
64 bytes from 172.17.30.99: icmp_seq=1 ttl=64 time=1.94 ms
64 bytes from 172.17.30.99: icmp_seq=2 ttl=64 time=0.860 ms
64 bytes from 172.17.30.99: icmp_seq=3 ttl=64 time=0.846 ms
64 bytes from 172.17.30.99: icmp_seq=4 ttl=64 time=0.577 ms

--- 172.17.30.99 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3001ms
rtt min/avg/max/mdev = 0.577/1.056/1.944/0.526 ms
```

可以看到现在到172.17.30.99的数据包会的通过k8s02上的docker0(这是由于我们上面配置了路由)，然后通过veth到br-tun，接着br-tun通过vxlan到k8s03上的br-tun，后者再通过veth到k8s03上的docker0，然后docker0桥内转发数据包给了debug-docker，后者通过veth发给了其peer口，也就是debug-host。如果大家这一步可以ping通的话我们的这个测试用的网络环境基本就可以了。

下面我们在k8s02及k8s03上下载Kubernetes需要的pause镜像。因为这个镜像是托管在google的gcr上的，后者被墙掉了，因此需要用下面的方法绕绕一下：
```
[root@k8s02 ~]# docker pull docker.io/kubernetes/pause
Trying to pull repository docker.io/kubernetes/pause ...
6c4579af347b: Download complete 
511136ea3c5a: Download complete 
e244e638e26e: Download complete 
Status: Downloaded newer image for docker.io/kubernetes/pause:latest
[root@k8s02 ~]# docker tag docker.io/kubernetes/pause gcr.io/google_containers/pause:0.8.0
```

然后再再k8s02及k8s03上面下个nginx的镜像用于下面的测试：
```
[root@k8s02 ~]# docker pull nginx
```

下面来试下用Kubernetes建立个service测试下我们的网络环境是否符合要求。相关的配置文件如下：
```
[root@k8s01 ~]# cat nginx-rc.yaml 
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-controller
spec:
  replicas: 1
  selector:
    app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
[root@k8s01 ~]# cat nginx-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  ports:
  - port: 8000
    targetPort: 80
    protocol: TCP
  selector:
    app: nginx
```
这里我们设置replicas为1，这个设置可以让我们观察我们的网络环境是否正常。

现在测试建立下这个rc和service，首先需要建立node。如果发现异常的话可以尝试重新启动下kubelet服务：
```
[root@k8s01 ~]# cat node01.json 
{
    "apiVersion": "v1",
    "kind": "Node",
    "metadata": {
        "name": "k8s02",
        "labels":{ "name": "kub-node-label"}
    },
    "spec": {
        "externalID": "k8s02"
    }
}
[root@k8s01 ~]# cat node02.json 
{
    "apiVersion": "v1",
    "kind": "Node",
    "metadata": {
        "name": "k8s03",
        "labels":{ "name": "kub-node-label"}
    },
    "spec": {
        "externalID": "k8s03"
    }
}
[root@k8s01 ~]# kubectl create -f ./node01.json
node "k8s02" created
[root@k8s01 ~]# kubectl create -f ./node02.json
node "k8s03" created
[root@k8s01 ~]# kubectl get nodes
NAME      LABELS                          STATUS
k8s01    kubernetes.io/hostname=k8s01   Ready
k8s02    name=kub-node-label             Ready
k8s03    name=kub-node-label             Ready
```

现在建立rc和service：
```
[root@k8s01 ~]# kubectl create -f ./nginx-rc.yaml 
replicationcontroller "nginx-controller" created
[root@k8s01 ~]# kubectl get rc
CONTROLLER         CONTAINER(S)   IMAGE(S)   SELECTOR    REPLICAS
nginx-controller   nginx          nginx      app=nginx   1
[root@k8s01 ~]# kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-controller-zfd64   1/1       Running   0          25s
[root@k8s01 ~]# kubectl create -f ./nginx-service.yaml 
service "nginx-service" created
[root@k8s01 ~]# kubectl get service
NAME            LABELS                                    SELECTOR    IP(S)           PORT(S)
kubernetes      component=apiserver,provider=kubernetes   <none>      10.254.0.1      443/TCP
nginx-service   <none>                                    app=nginx   10.254.85.157   8000/TCP
```

现在在k8s02及k8s03上curl下10.254.85.157:8000，发现都可以获取到nginx的欢迎页面：
```
# k8s02上
[root@k8s02 ~]# curl 10.254.85.157:8000
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

# k8s03上
[root@k8s03 ~]# curl 10.254.85.157:8000
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

同时观察容器运行情况，可以看到nginx目前是建立在了k8s03上：
```
# k8s02上
[root@k8s02 ~]# docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
[root@k8s02 ~]# 

# k8s03上
[root@k8s03 ~]# docker ps
CONTAINER ID        IMAGE                                  COMMAND                CREATED             STATUS              PORTS               NAMES
9ea0c512a8cb        nginx                                  "nginx -g 'daemon of   4 minutes ago       Up 4 minutes                            k8s_nginx.6420169f_nginx-controller-zfd64_default_eab9db09-40ca-11e5-b237-fa163ee101cd_f4ab7b51   
e4fb6592bfee        gcr.io/google_containers/pause:0.8.0   "/pause"               4 minutes ago       Up 4 minutes                            k8s_POD.ef28e851_nginx-controller-zfd64_default_eab9db09-40ca-11e5-b237-fa163ee101cd_19ed5eed     
[root@k8s03 ~]# 
```

可以看到k8s03上负责pod网络的pause也一起起来了，另外由于上面在k8s02上可以访问到我们的nginx，说明我们的ovs网络也起作用了。如果观察iptables规则的话可以发现service的vip的地址都被代理到了kube-proxy。此时我们的一个基于openvswitch的Kubernetes多节点环境就算是搭建好了。




关于Kubernetes就介绍这么多。笔者个人认为对于希望从事大规模的集群运维的读者来说，熟悉Kubernetes这类调度控制平台的设计思想要比熟悉Docker这类容器技术重要的多。