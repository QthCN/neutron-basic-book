## 内核网络相关基础知识

这一章主要是为了后面的章节做准备的，内容包括：

- sk_buff数据结构
- net_device数据结构
- 硬件中断&module机制
- softirq
- proc文件系统

关于这些方面的小秦所发现的最好的参考资料是《Understanding Linux Network Internals》。强烈建议每个对Linux内核中网络有兴趣的同学读下这本书。本书中与底层网络相关的资料很多都是参考着这本书而来的。由于本书的目标还是基础，因此对于很多内容都只是做个入门的介绍，如果读者感兴趣可以参考其它的书籍或内核代码进行深入的学习。

### sk_buff数据结构

sk_buff在内核的地位简单的说就是：它代表了一个数据包。

首先来个大致的介绍让大家感受下它的重要性吧。比如一个网卡收到了一个包，此时大家可以认为这个数据包对于网卡来说只是一段电信号或光信号，网卡收到这个包后，会先保存在网卡本地，然后会告诉内核说：“内核君，我这有个包，快来拿”。内核于是就会去取这个包，既然是内核去取，那么这个包肯定到了内核后就要放到内存中的某个数据结构里了，这个数据结果对于内核来说就是sk_buff。所以我们可以认为此时这个sk_buff数据结构包含了这个数据包的所有内容。内核得到了这个sk_buff后，接下来做的事情就是让它走协议栈，我们就把协议栈当成是一个有n层楼的大厦、sk_buff是一个携带了传单想要在楼顶散发传单的小伙好了。于是sk_buff这个小伙就开始爬大厦，可能它能一直爬过所有楼层爬到楼顶，也可能在某一层就因为迷路或者找错大厦之类的原因被原路返回（也就是要下楼了）或者因为非法发传单而被捕了。在它爬楼的过程中每一层楼的楼管都会对其要发的传单做些标注或修改，但不论怎样传单在小伙爬到楼顶散发前都是属于这个小伙的，楼管不管做什么操作也只是在这个小伙的传单上做操作，如果某层的某个居民想了解传单的内容，那么必须去找这个小伙了解。所以可以看到，sk_buff就是我们的数据包在从数据包到达内核以及这个包最终到达上层之间唯一拥有这个数据包内容的结构体。其包含了网卡收到的数据包的全部信息以及协议栈对这个数据包做出的相关附加信息。

下面来看下这个sk_buff结构体的几个重要属性，列出的属性中有些属性在后面的章节会成为主角：

```
struct sk_buff {
	......
	struct sk_buff *next; //指向下一个sk_buff
	struct sk_buff *prev; //指向前一个sk_buff
	
	struct sock *sk; //拥有这个sk_buff的socket
	
	struct net_device *dev; //发送或者接收这个数据包的设备
	
	unsigned int len; //数据包的真实长度
	
	unsigned char *head;	unsigned char *end;	unsigned char *data;	unsigned char *tail;
	
	__u32 priority;  //数据包的优先级
	
	__u8 pkt_type:3; //数据包的类型
	__be16 protocol; //驱动程序判定的这个数据包的协议类型
	......
}
```

一开始的例子中举例了小伙发传单爬大楼的例子，sk_buff在内核协议栈中就是这样一层一层走协议栈的。第一层可能是链路层（L2），然后爬到IP层（L3），再然后会爬到传输层（L4）。在走协议栈的过程中，sk_buff中的一些属性可能会被修改。先来说下上面列出的几个重要属性。

next/prev:
    
	内核中的所有sk_buff都被串在了一个双向链表里以便检索和管理。这两个指针分别指向双向链表的前者和后者。
    
sk:

	如果一个数据包是发给本机的或者是由本机生成的，那么一般会有上层应用接收或产生这个数据包。写过网络代码的人都知道目前Linux是通过socket来实现网络编程的，这个sk指针就指向了将要接收这个数据包或者是产生了这个数据包的socket。如果这个数据包是走转发路径的呢（比如我本机用来做路由器）？此时这个指针就是NULL。
	
dev:

	我们下面一节就会讲到这个结构体。net_device在内核中代表了一个网络设备，比如我们的eth0、lo设备等。在sk_buff中这个结构体指针指的是发送或接收这个数据包的设备。在我们下面的章节中，net_device这个结构体要远比sk_buff重要的多。
	
pkt_type:

	这个值是根据sk_buff的二层目的地址判定出的数据包的类型。所有的类型可以在include/linux/if_packet.h中找到。
	
protocol:

	protocol指的是三层的协议类型。一般是IP、IPv6以及ARP。这个值一般是在收到数据包后，由对应的网卡的驱动程序判断数据包的三层协议类型后设置的。在我们之后将收包流程的时候会看到这个值是如何被驱动设置的。
	
head/end/data/tail:

	这四个指针分别指向了sk_buff中的不同的数据段（可以把sk_buff中的其它熟悉看成是元信息，data或head指向的才是我们真实的数据包内容）。简单的说：head指向了我们的sk_buff的最开头，end指向了我们的sk_buff的最末尾。data指向了我们真正协议报文内容的最开头，tail指向了我们真正协议报文内容的最末尾。为什么会这么复杂呢？原因在于内存的分配、扩容操作是一个比较耗时的操作。在学校网络课的时候网络老师最有可能举的一个协议栈的例子就是邮局送信。我们写好信后，我们得将它放到信封里，信封上贴好邮票写上地址才能投递出去。投递的过程中信连着信封会放在邮局的小卡车上在城市间传输，传送到目的地后收件人会拆开信封，然后才会看到信的内容。对于协议栈也存在这么一个加信封、解信封的过程，在协议栈里信封就是我们的协议头。在内存中如果我们明确知道要分配多大的内存，那么我们直接分配一块连续内存就行了（可以理解这个内存就是我们的信的内容），但是由于我们有增加头部、移除头部（加信封、解信封）的过程，我们势必要在我们申请的内存的头部后者尾部再次申请内存。在已有的一块内存的头部或尾部再次附加一段内存对于内核来说可是一件比较痛苦的事情，所以内核的方法很简单：一次性就申请一大块足够大的内存，这块内存的头就是head指针指向的地方，尾就是end指向的地方。但这块内存中真正有意义的数据由data和tail来标示。比如一开始这块内存只存在TCP的数据，所以data会的指向TCP的头部，此时data和head之间的一大块空闲内存就还空着。然后这个包传到了IP层，IP也要给这个数据包加上IP的头部，于是data指针会往head指针靠近，其会的靠近IP头部的长度这么一块距离。
	
len:

	len指的是我们上面说的data指针与tail指针之间的差值，也就是数据包报文目前被协议栈认为的有意义的长度的大小。因此当sk_buff在协议栈之间传递的时候，随着data或tail指针的变化，其长度也会相应变化。
	
	
关于sk_buff我们就介绍这么多，对于我们接下来的内容来说，大家最重要的是要记住sk_buff代表了我们的数据包，在内核协议栈中数据包通过sk_buff这个数据结构来在协议栈之间传递。
	

### net_device数据结构

如果我们通过ifconfig或ip link show命令查看本机的网卡，我们可以看到类似如下的输出：

```
[root@dev ~]# ip link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:89:4a:1e brd ff:ff:ff:ff:ff:ff
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:77:2b:ca brd ff:ff:ff:ff:ff:ff
4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:66:66:88 brd ff:ff:ff:ff:ff:ff
5: enp0s10: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:5e:84:f9 brd ff:ff:ff:ff:ff:ff
6: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT 
    link/ether 52:54:00:27:e8:bc brd ff:ff:ff:ff:ff:ff
7: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN mode DEFAULT qlen 500
    link/ether 52:54:00:27:e8:bc brd ff:ff:ff:ff:ff:ff
```

这里我们的ip命令列出了本机的所有网卡，比如lo、enp0s3、virbr0等。其中即有lo这个回环设备，也有enp0s3这个真实的物理设备，还存在virbr0这个网桥。在内核中，这些都是由net_device这个结构体来表示的。一个net_device代表了一个网络设备，包括真实的物理设备、虚拟设备或者网桥等。当一个网络设备被驱动识别后，驱动就会在内核中建立一个net_device数据结构代表这个设备，之后在数据包的发送和接收的时候，这个设备都会作为重要的参数在各个函数之间传递（因此C没有对象的概念......）。我们来看下其重要的一些属性：

```
struct net_device {
	......
	char name[IFNAMSIZ]; //设备名
	
	int irq; //这个设备注册的中断向量号
	
	unsigned int flags; //用于表示这个设备的一些说明
	
	unsigned int mtu; //表示这个设备的mtu大小
	unsigned int type; //表示这个设备支持的网络类型，比如以太网
	unsigned long state; //表示这个设备的状态
	
	struct Qdisc *qdisc; //通过tc进行数据包的发送的时候会的用到这个
	
	const struct net_device_ops *netdev_ops; //这个设备所支持的大部分操作的函数实现
	const struct ethtool_ops *ethtool_ops; //这个设备所支持的ethtool操作的函数实现
	......
}
```

我们来看下这几个属性代表了什么。

name:

	name就是我们通过ip link show看到的lo、enp0s3这样的名字。
	
irq:

	在我们下面讲中断的时候会的讲到中断号的作用。简单的理解就是：当外部硬件设备发生了一件事情的时候，会需要某个方式通知内核，而对于内核来说其只会知道目前XXX发生了事情，而XXX就是个数字。于是内核去一个类似于Python字典结构中根据XXX去找对应的XXX的处理函数，然后被动的执行这个函数去处理这个事件。XXX就是我们这里的irq，如果中断没有共享的话这个irq和我们的net_device是唯一对应的，否则驱动程序要判断产生的中断事件是不是其自己所关心的。比如三个设备都是用10这个irq号，此时内核收到了一个中断号为10的事件，于是内核会依次调用这个三个设备的对应中断处理函数，每个函数要判断下这个事件是不是自己要处理的说（简单的说就是这个事件是不是自己设备产生的）。
	
flags:

	我们通过ip link show命令的时候可以看到这个设备的一些状态信息，这些信息就是由flags这个字段标记的（当然还有其他的字段表示其它的一些信息）。比如是否是处于混杂模式等。
	
mtu:

	mtu只的是这个设备所能传输的最大报文大小。注意和传输层的最大传输大小区分开来。
	
type:

	type表示这个网络设备的类型，比如以太网设备等。
	
state:

	state表示这个设备的状态。这个在后面我们会看到这个状态字段会影响我们的数据包能否被发送。比如如果state中__LINK_STATE_XOFF标记位为true的时候这个网卡就不能发送数据包啦。
	
qdisc:

	内核中发送数据包的时候会的走一个叫做tc（traffic control）的系统。简单的说就是一个数据包要被发送的时候，先扔到qdisc中去排队，然后再从qdisc中取出一个包，这个被取出的包才是要被发送的。至于这个包是不是我扔进去的那个包就不知道了，这得开tc配置的是什么算法了。tc在我们之前的章节中有过介绍，在之后看到发包的实现的时候大家就清楚其起作用的时间了。
	
netdev_ops:

	在解释这个之前先说下C的函数指针。在面向对象的语言中或多或少存在接口（interface）的概念。比如我一个Python类Animal，其存在一个方法是run。那么我可以有一个子类Dog继承这个Animal类，然后其必然拥有run这个方法。其可以通过覆盖父类自己实现run，或者直接使用父类的run，但是总之我们知道其肯定有run这个方法。因此通过我们之前文章中讲过的stevedore等方法我们可以动态的加载对应的driver而调用不同的实现。对于C语言来说其语言级别没有直接的接口这种特性，所以其通过函数指针的方式实现类似的功能。比如我们的结构体A拥有ops这个函数指针，然后我们写个注释说这个ops指向的函数会做打印当前主机内存的操作。然后我们在linux上跑这个程序，此时可以实现一个函数show_mem其实现为调用free方法查看内存大小。然后将ops指向这个函数，然后别人调用A的ops的时候就能看到内存大小了。同样的，在windows上我们的free方法就不管用了，于是有人在windows上也实现了一个windows版本的show_mem，然后将A结构体的ops指向这个windows版本的show_mem，此时上层调用A这个结构体的程序不用做任何修改，其调用A的ops方法依然可以正常获取到windows主机的内存大小。在net_device中这种需求就特别强烈了，比如收发包这个操作在网卡层面每家厂商都不一样，但内核的代码肯定不能根据不同的厂商动态变吧，所以内核的代码是通用的，其调用类似于A的ops方法，但是每个驱动可以实现不同的ops，于是对于驱动X，内核调用ops就会执行X的实现，对于驱动Y则内核调用ops就会执行Y的实现。netdev_ops就包含了内核需要的大部分数据包在网卡层面的收取、发送的函数指针。比如：ndo_change_mtu用于修改网卡的mtu，ndo_start_xmit用于发送数据包等。
	
ethtool_ops:

	ethtool_ops也是一个包含了大量函数指针的结构体，和netdev_ops的区别在于其主要用于实现ethtool的各种功能。在这里ethtool就是我们的上层应用，通过函数指针将其和底层实现解耦，因此内核的同一套ethtool代码对于不同的net_device可以根据驱动实现相同的功能，而不用在乎底层不同设备的不同。
	
关于net_device我们在大致了解了它一下后就先告一段落，后面我们会再讲到它，在本章中它是我们的第一主角。目前我们只要知道，它代表了一个网络设备，其包含了这个网络设备的所有内核需要的通用方法的具体实现（通过函数指针）。	


### 硬件中断&module机制

上面在讲net_device的时候讲到了它有一个属性irq代表了中断向量号，那么这里的中断是什么意思呢？先来看一个问题：如果我在键盘上敲下了一个键，那么我的内核是怎么知道键盘这个外部设备被敲下了这个键并做出响应的（比如在屏幕上显示这个键对应的字母）？在一般的事件驱动的软件开发中，我们一般都是以一个while循环来查看是否有新的事件产生，代码类似如下：

```
while True:
	event = get_event()
	if event.get("type") == "数字键1":
		print("1")
	elif event.get("type") == "数字键2":
		print("2")
	......
	sleep(1)
```

对于这个事件驱动的模型存在一个问题，比如我print("1")的这个操作，如果其会花费30秒，那么我整个while循环就卡在这个print上了。在这30秒内我敲入的其它键内核可能就忽略掉了。另外可以看到这里一般会的有一个sleep操作，如果sleep期间产生了事件那么这个事件也得等sleep白白结束后才会被获取。因此这个模型确实管用，但其对事件的响应的及时性并不好，我们可以把它看成是一种轮询的模型。现在我们来看下内核的中断模型，并看下其和这个轮询模型的区别。

首先先说下，中断这个得CPU支持（其实内核设计成现在这样，很大一部分原因是CPU只能支持这样的设计，在这里是硬件决定软件，基础决定上层）。简单的说，我们的键盘连在了一块中断芯片上，然后这个中断芯片连在CPU上。当我们敲下一个键的时候，键盘产生一个电信号到中断芯片，中断芯片根据事先编程的规则，根据编程的逻辑对这个电信号做个转换，然后发送给CPU。CPU上接中断芯片的引脚对于CPU来说有特殊的含义：当这个引脚上产生电信号的时候，CPU会立刻中断并存储当前的执行环境，然后查询中断寄存器，根据引脚传过来的中断irq查看中断寄存器指向的中断向量表中偏移为irq的中断处理函数是什么，并立刻调用这个处理函数进行处理。然后内核就开始处理这个按键事件了，比如中断处理函数就是在屏幕上输出一些东西。在中断处理函数执行完成后内核会通过特定的汇编指令返回之前的上下文，因此会继续执行之前的内容。这就是一次最简单的中断事件的处理过程了。

当然实际情况会复杂一些，比如我们可以设置屏蔽中断，或者设置内核不响应中断等等。另外上面也提到过中断号共享，因为我们看到了中断需要中断芯片的支持，中断芯片支持的中断向量号是有限的，如果我们的设备数大于这个数目中断向量号就可能不够，此时多个设备可以共享一个中断向量号。

另外我们也看到了轮询模型，我们来比较下这两个模型。对于中断模型其存在一些缺点，打个比方当我网卡不停的收到大量数据包的时候，每个数据包都会触发一次中断，而我的内核还没处理完上一个数据包呢就被新的中断给打断了，这样就造成了一个恶性循环，内核永远无法处理完数据包，且大量的CPU时间都被响应中断时间给占用了。此时轮询模式就管用了，当内核感觉到数据包量特别大的时候内核可以通过轮询的方式去主动询问网卡而不是由网卡中断内核来获取数据包。这样只要内核没有处理完当前的数据包其就不会去问网卡要新的数据包，只有当前的数据包都处理完后，才会调用类似上面例子的get_event函数去从网卡一次性获取多个数据包然后进行处理。并且由于不处于中断的上下文，如果此时用于处理数据包所消耗的CPU时间过多，那么其可以主动让出CPU给其它进程使用。当然中断模型的优点也很明显，对于数据包量不大的情况，轮询模型的延迟会大于中断模型。轮询模型在我们下面讲收包的时候会讲到，内核中有NAPI支持对网卡的轮询。

除了最简单的中断和轮询外，还有一些别的方法，比如驱动发现数据包量很大的时候就不每个数据包都立刻产生一次中断了，其可能会等到连续有30个包后才一次性发个中断，这样可以减少中断的次数。另外也可以等待一定特定时间后才发送一个中断，也能一定程度上减少中断的次数。不过缺点是都会造成延迟。

有一个需要大家记在心里的东西：中断是和CPU的核绑定的。比如我的主机有32个核，那么可以简单的近似认为我有32个独立的中断芯片。对于设备我只能同一时间接在一个芯片上，换句话说如果我的这个设备接在了第一个核的芯片上，那么每次这个设备产生中断只有第一个核会响应，其它的核则感知不到这个事件。这也是为什么在以前没有网卡多队列或RPS的时候我们会发现CPU0的软中断时间特别高而其余核很空闲的原因，因为所有的网卡中断只有一个核能处理。

硬件中断我们就暂时讲这么多，我们接下来会写一个简单的硬件中断的例子，但是在此之前还要先介绍下内核的module机制。这里又要说些内核或操作系统的历史了。

我们都知道操作系统是个很复杂的大程序，为了设计这么一个大程序，操作系统的设计者们将操作系统分成了很多很多的子系统，比如IO子系统，内存子系统，网络协议栈子系统等等。那么这些子系统如何组合在一起呢？有两种主流的方法。第一种是说我把我的操作系统内核设计的功能非常简单，只将非常核心非常必要的功能放在一起，其余的子系统我通过进程或线程的方式提供，当子系统之间要交互的时候可以通过如IPC之类的方法进行交互。第二种则是将所有的功能都编译在同一个操作系统可执行文件中，就是一个单独的进程。一般将前者称为微内核（也就是说内核的真正核心只提供核心且非常必要的少数功能），后者称为宏内核（也就是说所有的东西都塞在一起）。学术界貌似比较喜欢微内核，原因是这种架构的子系统替换或升级比较简单，只要停止原来的进程启动新的就行了（这样很便于他们研究），目前Windows的内核就是微内核的代表。但是对于linux则是采用的宏内核，所以在很久之前linux的设计者linus还和一些其他的知名内核设计者有过分歧，他坚持认为微内核进程间的通信会造成效率低下等问题，在他的坚持下linux目前是以宏内核的方式存在的。不过宏内核确实存在不足，比如我一个研究人员想试一下一个新的驱动，那我得重新编译一个完整的内核才行，所以linux在宏内核的基础下引入了module机制。

