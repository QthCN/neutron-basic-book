## 内核网络相关基础知识

这一章主要是为了后面的章节做准备的，内容包括：

- sk_buff数据结构
- net_device数据结构
- 硬件中断&module机制
- softirq
- proc文件系统

关于这些方面的小秦所发现的最好的参考资料是《Understanding Linux Network Internals》。强烈建议每个对Linux内核中网络有兴趣的同学读下这本书。本书中与底层网络相关的资料很多都是参考着这本书而来的。由于本书的目标还是基础，因此对于很多内容都只是做个入门的介绍，如果读者感兴趣可以参考其它的书籍或内核代码进行深入的学习。

### sk_buff数据结构

sk_buff在内核的地位简单的说就是：它代表了一个数据包。

首先来个大致的介绍让大家感受下它的重要性吧。比如一个网卡收到了一个包，此时大家可以认为这个数据包对于网卡来说只是一段电信号或光信号，网卡收到这个包后，会先保存在网卡本地，然后会告诉内核说：“内核君，我这有个包，快来拿”。内核于是就会去取这个包，既然是内核去取，那么这个包肯定到了内核后就要放到内存中的某个数据结构里了，这个数据结果对于内核来说就是sk_buff。所以我们可以认为此时这个sk_buff数据结构包含了这个数据包的所有内容。内核得到了这个sk_buff后，接下来做的事情就是让它走协议栈，我们就把协议栈当成是一个有n层楼的大厦、sk_buff是一个携带了传单想要在楼顶散发传单的小伙好了。于是sk_buff这个小伙就开始爬大厦，可能它能一直爬过所有楼层爬到楼顶，也可能在某一层就因为迷路或者找错大厦之类的原因被原路返回（也就是要下楼了）或者因为非法发传单而被捕了。在它爬楼的过程中每一层楼的楼管都会对其要发的传单做些标注或修改，但不论怎样传单在小伙爬到楼顶散发前都是属于这个小伙的，楼管不管做什么操作也只是在这个小伙的传单上做操作，如果某层的某个居民想了解传单的内容，那么必须去找这个小伙了解。所以可以看到，sk_buff就是我们的数据包在从数据包到达内核以及这个包最终到达上层之间唯一拥有这个数据包内容的结构体。其包含了网卡收到的数据包的全部信息以及协议栈对这个数据包做出的相关附加信息。

下面来看下这个sk_buff结构体的几个重要属性，列出的属性中有些属性在后面的章节会成为主角：

```
struct sk_buff {
	......
	struct sk_buff *next; //指向下一个sk_buff
	struct sk_buff *prev; //指向前一个sk_buff
	
	struct sock *sk; //拥有这个sk_buff的socket
	
	struct net_device *dev; //发送或者接收这个数据包的设备
	
	unsigned int len; //数据包的真实长度
	
	unsigned char *head;	unsigned char *end;	unsigned char *data;	unsigned char *tail;
	
	__u32 priority;  //数据包的优先级
	
	__u8 pkt_type:3; //数据包的类型
	__be16 protocol; //驱动程序判定的这个数据包的协议类型
	......
}
```

一开始的例子中举例了小伙发传单爬大楼的例子，sk_buff在内核协议栈中就是这样一层一层走协议栈的。第一层可能是链路层（L2），然后爬到IP层（L3），再然后会爬到传输层（L4）。在走协议栈的过程中，sk_buff中的一些属性可能会被修改。先来说下上面列出的几个重要属性。

next/prev:
    
	内核中的所有sk_buff都被串在了一个双向链表里以便检索和管理。这两个指针分别指向双向链表的前者和后者。
    
sk:

	如果一个数据包是发给本机的或者是由本机生成的，那么一般会有上层应用接收或产生这个数据包。写过网络代码的人都知道目前Linux是通过socket来实现网络编程的，这个sk指针就指向了将要接收这个数据包或者是产生了这个数据包的socket。如果这个数据包是走转发路径的呢（比如我本机用来做路由器）？此时这个指针就是NULL。
	
dev:

	我们下面一节就会讲到这个结构体。net_device在内核中代表了一个网络设备，比如我们的eth0、lo设备等。在sk_buff中这个结构体指针指的是发送或接收这个数据包的设备。在我们下面的章节中，net_device这个结构体要远比sk_buff重要的多。
	
pkt_type:

	这个值是根据sk_buff的二层目的地址判定出的数据包的类型。所有的类型可以在include/linux/if_packet.h中找到。
	
protocol:

	protocol指的是三层的协议类型。一般是IP、IPv6以及ARP。这个值一般是在收到数据包后，由对应的网卡的驱动程序判断数据包的三层协议类型后设置的。在我们之后将收包流程的时候会看到这个值是如何被驱动设置的。
	
head/end/data/tail:

	这四个指针分别指向了sk_buff中的不同的数据段（可以把sk_buff中的其它熟悉看成是元信息，data或head指向的才是我们真实的数据包内容）。简单的说：head指向了我们的sk_buff的最开头，end指向了我们的sk_buff的最末尾。data指向了我们真正协议报文内容的最开头，tail指向了我们真正协议报文内容的最末尾。为什么会这么复杂呢？原因在于内存的分配、扩容操作是一个比较耗时的操作。在学校网络课的时候网络老师最有可能举的一个协议栈的例子就是邮局送信。我们写好信后，我们得将它放到信封里，信封上贴好邮票写上地址才能投递出去。投递的过程中信连着信封会放在邮局的小卡车上在城市间传输，传送到目的地后收件人会拆开信封，然后才会看到信的内容。对于协议栈也存在这么一个加信封、解信封的过程，在协议栈里信封就是我们的协议头。在内存中如果我们明确知道要分配多大的内存，那么我们直接分配一块连续内存就行了（可以理解这个内存就是我们的信的内容），但是由于我们有增加头部、移除头部（加信封、解信封）的过程，我们势必要在我们申请的内存的头部后者尾部再次申请内存。在已有的一块内存的头部或尾部再次附加一段内存对于内核来说可是一件比较痛苦的事情，所以内核的方法很简单：一次性就申请一大块足够大的内存，这块内存的头就是head指针指向的地方，尾就是end指向的地方。但这块内存中真正有意义的数据由data和tail来标示。比如一开始这块内存只存在TCP的数据，所以data会的指向TCP的头部，此时data和head之间的一大块空闲内存就还空着。然后这个包传到了IP层，IP也要给这个数据包加上IP的头部，于是data指针会往head指针靠近，其会的靠近IP头部的长度这么一块距离。
	
len:

	len指的是我们上面说的data指针与tail指针之间的差值，也就是数据包报文目前被协议栈认为的有意义的长度的大小。因此当sk_buff在协议栈之间传递的时候，随着data或tail指针的变化，其长度也会相应变化。
	
	
关于sk_buff我们就介绍这么多，对于我们接下来的内容来说，大家最重要的是要记住sk_buff代表了我们的数据包，在内核协议栈中数据包通过sk_buff这个数据结构来在协议栈之间传递。
	

### net_device数据结构

如果我们通过ifconfig或ip link show命令查看本机的网卡，我们可以看到类似如下的输出：

```
[root@dev ~]# ip link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:89:4a:1e brd ff:ff:ff:ff:ff:ff
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:77:2b:ca brd ff:ff:ff:ff:ff:ff
4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:66:66:88 brd ff:ff:ff:ff:ff:ff
5: enp0s10: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:5e:84:f9 brd ff:ff:ff:ff:ff:ff
6: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT 
    link/ether 52:54:00:27:e8:bc brd ff:ff:ff:ff:ff:ff
7: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN mode DEFAULT qlen 500
    link/ether 52:54:00:27:e8:bc brd ff:ff:ff:ff:ff:ff
```

这里我们的ip命令列出了本机的所有网卡，比如lo、enp0s3、virbr0等。其中即有lo这个回环设备，也有enp0s3这个真实的物理设备，还存在virbr0这个网桥。在内核中，这些都是由net_device这个结构体来表示的。一个net_device代表了一个网络设备，包括真实的物理设备、虚拟设备或者网桥等。当一个网络设备被驱动识别后，驱动就会在内核中建立一个net_device数据结构代表这个设备，之后在数据包的发送和接收的时候，这个设备都会作为重要的参数在各个函数之间传递（因此C没有对象的概念......）。我们来看下其重要的一些属性：

```
struct net_device {
	......
	char name[IFNAMSIZ]; //设备名
	
	int irq; //这个设备注册的中断向量号
	
	unsigned int flags; //用于表示这个设备的一些说明
	
	unsigned int mtu; //表示这个设备的mtu大小
	unsigned int type; //表示这个设备支持的网络类型，比如以太网
	unsigned long state; //表示这个设备的状态
	
	struct Qdisc *qdisc; //通过tc进行数据包的发送的时候会的用到这个
	
	const struct net_device_ops *netdev_ops; //这个设备所支持的大部分操作的函数实现
	const struct ethtool_ops *ethtool_ops; //这个设备所支持的ethtool操作的函数实现
	......
}
```

我们来看下这几个属性代表了什么。

name:

	name就是我们通过ip link show看到的lo、enp0s3这样的名字。
	
irq:

	在我们下面讲中断的时候会的讲到中断号的作用。简单的理解就是：当外部硬件设备发生了一件事情的时候，会需要某个方式通知内核，而对于内核来说其只会知道目前XXX发生了事情，而XXX就是个数字。于是内核去一个类似于Python字典结构中根据XXX去找对应的XXX的处理函数，然后被动的执行这个函数去处理这个事件。XXX就是我们这里的irq，如果中断没有共享的话这个irq和我们的net_device是唯一对应的，否则驱动程序要判断产生的中断事件是不是其自己所关心的。比如三个设备都是用10这个irq号，此时内核收到了一个中断号为10的事件，于是内核会依次调用这个三个设备的对应中断处理函数，每个函数要判断下这个事件是不是自己要处理的说（简单的说就是这个事件是不是自己设备产生的）。
	
flags:

	我们通过ip link show命令的时候可以看到这个设备的一些状态信息，这些信息就是由flags这个字段标记的（当然还有其他的字段表示其它的一些信息）。比如是否是处于混杂模式等。
	
mtu:

	mtu只的是这个设备所能传输的最大报文大小。注意和传输层的最大传输大小区分开来。
	
type:

	type表示这个网络设备的类型，比如以太网设备等。
	
state:

	state表示这个设备的状态。这个在后面我们会看到这个状态字段会影响我们的数据包能否被发送。比如如果state中__LINK_STATE_XOFF标记位为true的时候这个网卡就不能发送数据包啦。
	
qdisc:

	内核中发送数据包的时候会的走一个叫做tc（traffic control）的系统。简单的说就是一个数据包要被发送的时候，先扔到qdisc中去排队，然后再从qdisc中取出一个包，这个被取出的包才是要被发送的。至于这个包是不是我扔进去的那个包就不知道了，这得开tc配置的是什么算法了。tc在我们之前的章节中有过介绍，在之后看到发包的实现的时候大家就清楚其起作用的时间了。
	
netdev_ops:

	在解释这个之前先说下C的函数指针。在面向对象的语言中或多或少存在接口（interface）的概念。比如我一个Python类Animal，其存在一个方法是run。那么我可以有一个子类Dog继承这个Animal类，然后其必然拥有run这个方法。其可以通过覆盖父类自己实现run，或者直接使用父类的run，但是总之我们知道其肯定有run这个方法。因此通过我们之前文章中讲过的stevedore等方法我们可以动态的加载对应的driver而调用不同的实现。对于C语言来说其语言级别没有直接的接口这种特性，所以其通过函数指针的方式实现类似的功能。比如我们的结构体A拥有ops这个函数指针，然后我们写个注释说这个ops指向的函数会做打印当前主机内存的操作。然后我们在linux上跑这个程序，此时可以实现一个函数show_mem其实现为调用free方法查看内存大小。然后将ops指向这个函数，然后别人调用A的ops的时候就能看到内存大小了。同样的，在windows上我们的free方法就不管用了，于是有人在windows上也实现了一个windows版本的show_mem，然后将A结构体的ops指向这个windows版本的show_mem，此时上层调用A这个结构体的程序不用做任何修改，其调用A的ops方法依然可以正常获取到windows主机的内存大小。在net_device中这种需求就特别强烈了，比如收发包这个操作在网卡层面每家厂商都不一样，但内核的代码肯定不能根据不同的厂商动态变吧，所以内核的代码是通用的，其调用类似于A的ops方法，但是每个驱动可以实现不同的ops，于是对于驱动X，内核调用ops就会执行X的实现，对于驱动Y则内核调用ops就会执行Y的实现。netdev_ops就包含了内核需要的大部分数据包在网卡层面的收取、发送的函数指针。比如：ndo_change_mtu用于修改网卡的mtu，ndo_start_xmit用于发送数据包等。
	
ethtool_ops:

	ethtool_ops也是一个包含了大量函数指针的结构体，和netdev_ops的区别在于其主要用于实现ethtool的各种功能。在这里ethtool就是我们的上层应用，通过函数指针将其和底层实现解耦，因此内核的同一套ethtool代码对于不同的net_device可以根据驱动实现相同的功能，而不用在乎底层不同设备的不同。
	
关于net_device我们在大致了解了它一下后就先告一段落，后面我们会再讲到它，在本章中它是我们的第一主角。目前我们只要知道，它代表了一个网络设备，其包含了这个网络设备的所有内核需要的通用方法的具体实现（通过函数指针）。	


### 硬件中断&module机制

上面在讲net_device的时候讲到了它有一个属性irq代表了中断向量号，那么这里的中断是什么意思呢？先来看一个问题：如果我在键盘上敲下了一个键，那么我的内核是怎么知道键盘这个外部设备被敲下了这个键并做出响应的（比如在屏幕上显示这个键对应的字母）？在一般的事件驱动的软件开发中，我们一般都是以一个while循环来查看是否有新的事件产生，代码类似如下：

```
while True:
	event = get_event()
	if event.get("type") == "数字键1":
		print("1")
	elif event.get("type") == "数字键2":
		print("2")
	......
	sleep(1)
```

对于这个事件驱动的模型存在一个问题，比如我print("1")的这个操作，如果其会花费30秒，那么我整个while循环就卡在这个print上了。在这30秒内我敲入的其它键内核可能就忽略掉了。另外可以看到这里一般会的有一个sleep操作，如果sleep期间产生了事件那么这个事件也得等sleep白白结束后才会被获取。因此这个模型确实管用，但其对事件的响应的及时性并不好，我们可以把它看成是一种轮询的模型。现在我们来看下内核的中断模型，并看下其和这个轮询模型的区别。

首先先说下，中断这个得CPU支持（其实内核设计成现在这样，很大一部分原因是CPU只能支持这样的设计，在这里是硬件决定软件，基础决定上层）。简单的说，我们的键盘连在了一块中断芯片上，然后这个中断芯片连在CPU上。当我们敲下一个键的时候，键盘产生一个电信号到中断芯片，中断芯片根据事先编程的规则，根据编程的逻辑对这个电信号做个转换，然后发送给CPU。CPU上接中断芯片的引脚对于CPU来说有特殊的含义：当这个引脚上产生电信号的时候，CPU会立刻中断并存储当前的执行环境，然后查询中断寄存器，根据引脚传过来的中断irq查看中断寄存器指向的中断向量表中偏移为irq的中断处理函数是什么，并立刻调用这个处理函数进行处理。然后内核就开始处理这个按键事件了，比如中断处理函数就是在屏幕上输出一些东西。在中断处理函数执行完成后内核会通过特定的汇编指令返回之前的上下文，因此会继续执行之前的内容。这就是一次最简单的中断事件的处理过程了。

当然实际情况会复杂一些，比如我们可以设置屏蔽中断，或者设置内核不响应中断等等。另外上面也提到过中断号共享，因为我们看到了中断需要中断芯片的支持，中断芯片支持的中断向量号是有限的，如果我们的设备数大于这个数目中断向量号就可能不够，此时多个设备可以共享一个中断向量号。

另外我们也看到了轮询模型，我们来比较下这两个模型。对于中断模型其存在一些缺点，打个比方当我网卡不停的收到大量数据包的时候，每个数据包都会触发一次中断，而我的内核还没处理完上一个数据包呢就被新的中断给打断了，这样就造成了一个恶性循环，内核永远无法处理完数据包，且大量的CPU时间都被响应中断时间给占用了。此时轮询模式就管用了，当内核感觉到数据包量特别大的时候内核可以通过轮询的方式去主动询问网卡而不是由网卡中断内核来获取数据包。这样只要内核没有处理完当前的数据包其就不会去问网卡要新的数据包，只有当前的数据包都处理完后，才会调用类似上面例子的get_event函数去从网卡一次性获取多个数据包然后进行处理。并且由于不处于中断的上下文，如果此时用于处理数据包所消耗的CPU时间过多，那么其可以主动让出CPU给其它进程使用。当然中断模型的优点也很明显，对于数据包量不大的情况，轮询模型的延迟会大于中断模型。轮询模型在我们下面讲收包的时候会讲到，内核中有NAPI支持对网卡的轮询。

除了最简单的中断和轮询外，还有一些别的方法，比如驱动发现数据包量很大的时候就不每个数据包都立刻产生一次中断了，其可能会等到连续有30个包后才一次性发个中断，这样可以减少中断的次数。另外也可以等待一定特定时间后才发送一个中断，也能一定程度上减少中断的次数。不过缺点是都会造成延迟。

有一个需要大家记在心里的东西：中断是和CPU的核绑定的。比如我的主机有32个核，那么可以简单的近似认为我有32个独立的中断芯片。对于设备我只能同一时间接在一个芯片上，换句话说如果我的这个设备接在了第一个核的芯片上，那么每次这个设备产生中断只有第一个核会响应，其它的核则感知不到这个事件。这也是为什么在以前没有网卡多队列或RPS的时候我们会发现CPU0的软中断时间特别高而其余核很空闲的原因，因为所有的网卡中断只有一个核能处理。

硬件中断我们就暂时讲这么多，接下来介绍下内核的module机制。这里需要说些内核或操作系统的历史了。

我们都知道操作系统是个很复杂的大程序，为了设计这么一个大程序，操作系统的设计者们将操作系统分成了很多很多的子系统，比如IO子系统，内存子系统，网络协议栈子系统等等。那么这些子系统如何组合在一起呢？有两种主流的方法。第一种是说我把我的操作系统内核设计的功能非常简单，只将非常核心非常必要的功能放在一起，其余的子系统我通过进程或线程的方式提供，当子系统之间要交互的时候可以通过如IPC之类的方法进行交互。第二种则是将所有的功能都编译在同一个操作系统可执行文件中，就是一个单独的进程。一般将前者称为微内核（也就是说内核的真正核心只提供核心且非常必要的少数功能），后者称为宏内核（也就是说所有的东西都塞在一起）。学术界貌似比较喜欢微内核，原因是这种架构的子系统替换或升级比较简单，只要停止原来的进程启动新的就行了（这样很便于他们研究），目前Windows的内核就是微内核的代表。但是对于linux则是采用的宏内核，所以在很久之前linux的设计者linus还和一些其他的知名内核设计者有过分歧，他坚持认为微内核进程间的通信会造成效率低下等问题，在他的坚持下linux目前是以宏内核的方式存在的。不过宏内核确实存在不足，比如我一个研究人员想试一下一个新的驱动，那我得重新编译一个完整的内核才行，所以linux在宏内核的基础下引入了module机制。关于宏内核和微内核的知识推荐大家可以看下操作系统的书籍。

现在来简单介绍下module机制，这个是我们之后会讲到的动态加载驱动程序的基础。module机制对于Python来说可以想象成我动态的通过imp模块加载一个python文件，然后加载入内存。之后就可以自动调用了。比如：

```
#加载模块，file_path类似于dog.py，module_name为dog
modules = dict()
mod = imp.load_source(module_name, file_path)
modules[module_name] = mod

```

然后我们的dog.py中有一个类，继承基类的run方法并覆盖之：

```
class Dog(Animal):

	def run():
		print("Dog run run run.")
```

此时当我的主程序想要调用时，直接调用如下代码即可：

```
modules.get("dog").run()
```

那如果我想换个dog呢，很简单，我加载一个不同的dog文件，比如bad_dog.py，module_name依然为dog：

```
#这里file_path变为bad_dog.py
mod = imp.load_source(module_name, file_path)
modules[module_name] = mod
```

bad_dog.py代码如下：

```
class BadDog(Animal):

	def run():
		print("Bad Dog run run run.")
```

此时主程序再次调用时就会输出bad dog的run了。

内核中的module机制可以看成是类似的机制，只不过会更加复杂一些，并且需要用到编译、链接的一些知识及技巧，关于编译和链接，强烈推荐大家看下《程序员的自我修养----链接、编译与库》这本书，我们这里就不介绍了。我们可以将dog这个module_name看成是某个网卡的驱动，通过上面动态加载python文件的方式我们可以改变这个驱动的实际执行代码。下面是一个C语言版的真实的module代码，代码来自《Linux 设备驱动程序》：


```
#include <linux/init.h>
#include <linux/module.h>
MODULE_LICENSE("Dual BSD/GPL");

static int hello_init(void)
{
	printk(KERN_ALERT "Hello, world\n");
}

static void hello_exit(void)
{
	printk(KERN_ALERT "Goodbye, cruel world\n");
}

module_init(hello_init);
module_exit(hello_exit);
```

我们上面的例子中python文件加载就是直接加载，不会的调用什么方法，但是内核的module机制中提供了很多hook，比如这里的init和exit方法。我们完全可以在init中做足够的初始化工作，比如安装驱动，生成net_device设备等。当然如果要让我们上面的python代码页支持这种加载和卸载时的hook的话也很简单，读者可以自己仿照着改一改。

当我们写出这个文件后，编译它可以生成一个helloworld.ko的内核模块。然后我们可以通过insmod以及rmmod的方法加载、卸载这个模块。很容易可以猜到当我们insmod的时候内核会调用hello_init方法，而rmmod的时候内核会调用hello_exit方法，这一切全靠module_init和module_exit的帮助，它们提供了足够的符号信息让内核能正确定位模块中的init和exit方法。

举个简单的例子，比如我们的hello_init为eth0网卡设置了对应的net_device结构，并填充了各种ops函数，则内核在收发数据包的时候就会使用这里hello_init给eth0设置的ops函数。然后我们突然想换一个驱动了，于是我们调用rmmod卸载hello这个驱动，此时hello_exit就被调用，其做了清理工作，比如移除了net_device结构体。然后我们再用insmod加载一个别的驱动模块，此时我们的eth0就能在新的驱动程序下工作了。原理，就类似于我们的dog以及bad dog的例子，本质上是分层思想在起松耦合的作用。

关于硬件中断和module机制我们就讲这么多，大家只要记住外部设备如网卡会的通过发送中断信号的方式让内核感知到有事件发生了，然后内核就会通过中断号调用对应的中断处理函数。而module机制则是一个动态加载驱动或子系统的内核的一种机制，通过这种机制可以在不重新编译或重启内核的情况下实现驱动或子系统的更新。


### softirq

下面我们来讲softirq，softirq是中断的下半区的一种，所以我们先来讲中断的下半区。

上面我们讲过，外部设备通过硬件中断的方式让内核知晓有事件产生，然后内核会根据中断号调用对应的中断处理函数进行处理。我们将内核处理中断的这个过程称为中断上下文，中断上下文包括调用中断处理函数及处理函数的执行的过程。在中断上下文中，内核无法响应其它中断（或者说内核会的忽略其它中断）。从这里可以看出中断上下文必须极短才行，否则会的影响内核的正常工作。比如我们的进程间的切换就是通过定时器芯片定期的产生定时中断事件，内核得到这个中断事件后切换到对应的中断处理函数进行进程上下文的切换。想象一下如果一个中断处理函数需要30秒的时间，那么这30秒的时间内核不能响应任何其它中断，所以我们的进程也就不能切换，新收到的数据包也无法正常处理，这会严重影响我们的内核的正常工作。

那么如何解决这个问题呢？中断事件是必须响应的，因为这个是内核目前的最基本的获取外部事件的方式，所以中断上下文就一定会存在。能不能在中断上下文中响应其它中断呢？不大可行，最容易列出的缺点就是这样做会的极大的增加开发者的负担，开发者在开发中断处理函数的时候得时时挂念着自己的代码随时会被中断等等问题（本来内核就很复杂了）。所以内核提供了一种下半区的机制来解决这个问题。

下半区的思想很简单，既然中断上下文对时间的要求非常敏感，那么在中断上下文期间我就只把必须做的事情做了，然后剩下的耗时的工作我们就在内核的某个变量中记录下，记录内容类似：“内核，这里有点事，你有空的时候来继续完成”，然后我们的中断上下文就退出。等到某个合适的时机，比如内核开始空闲的时候，内核就会的去查看是不是有要做的事情，如果发现有要做的事情那么内核就会去处理这些事情，注意此时内核在处理这些事情的时候内核是可以响应其它中断的。那么在哪些时机内核会的去检查是不是有要做的事情呢？一种时机是内核进行进程间切换的时候（也就是在schedule函数执行的时候），内核会做这样的检查。

下半区的概念我们清楚后，我们来说下内核的支持下半区的具体实现（下半区只是个思想，这里将的是支持这种思想的实现）。目前内核提供了多种实现，softirq是其中的一种，由于我们的协议栈的实现主要是通过softirq来做下半区的处理的，所以我们这里只讲softirq。

网上很多资料讲softirq翻译成软件中断或软中断，由于这个翻译会的和int 80这种程序发起的中断造成混淆，所以这里小秦还是将它称为softirq。

目前softirq支持处理下面这些类型的下半区工作：

```enum {         HI_SOFTIRQ=0,         TIMER_SOFTIRQ,         NET_TX_SOFTIRQ,         NET_RX_SOFTIRQ,         SCSI_SOFTIRQ,         TASKLET_SOFTIRQ};
```

这里NET_TX_SOFTIRQ是处理发包的下半区，而NET_RX_SOFTIRQ是处理收包的下半区。举个例子，当一个数据包到达的时候，网卡触发了某个中断通知内核，内核在收到这个中断后，调用收包的中断处理函数，进入中断上下文，中断上下文在我们的例子里做了一个最简单的操作，就是将数据包从网卡拷贝到内核的sk_buff结构体中，然后内核做了个标记说我这里收到了个包（简单的说就是将某个全局变量的NET_RX_SOFTIRQ这一位标记为1），等有空的时候记得回来处理这个包。然后中断上下文就退出了。之后在某个时机比如上面说的进程切换的时候，内核会检查是否有标记说有数据包等着被处理，此时内核发现了之前留下的标记，于是调用某个函数开始处理这个数据包。

内核检查标记位的时候是按照我们上面列出来的顺序来检查的，也就是说如果TIMER_SOFTIRQ和NET_TX_SOFTIRQ都被标记了，那么内核会先处理TIMER_SOFTIRQ然后再处理NET_TX_SOFTIRQ。换句话说就是这里的顺序决定了下半区中等着被处理的任务的处理优先级。

那么内核怎么知道应该调用哪个函数来处理我们上面例子中的NET_RX_SOFTIRQ呢？通过open_softirq这个函数。比如open_softirq(TASKLET_SOFTIRQ, tasklet_action, NULL)就能将TASKLET_SOFTIRQ和tasklet_action这个处理函数绑定起来。

另外内核保证某个一核上只能执行一个softirq。简单的说就是在执行softirq的时候内核会通过加锁的方式保证不会同时运行多个softirq，比如第一个下半区处理函数在执行的时候被中断打断了，并且这个中断是用来切换进程的，于是又有机会调用softirq的入口函数，但是此时这次调用就会失败了，因为之前已经在运行softirq了。

我们再来继续看下内核调用收包的下半区的处理函数后，softirq会有什么行为。我们上面说过在最简单的情况下，在中断上下文的时候内核会将数据从网卡拷贝出来放到sk_buff中，那么这个sk_buff肯定是要放到某个链表或类似数据结构中的，这个链表的头被谁拥有呢？链表的头在内核中由softnet_data拥有，系统启动的时候内核会对每个CPU核初始化一个softnet_data结构体，所以如果某个收包的中断是在CPU0上接收的，那么这个数据包会放在softnet_data结构体中的input_pkt_queue链表中。然后在下半区处理函数执行的时候，其就会去这个链表上取数据包（也就是取sk_buff），然后调用某个函数让这个数据包走协议栈。一切看起来都很简单，但实际情况却会复杂一些。我们上面说过在softirq执行期间，内核是可以响应中断的，所以可能softirq在执行完这个数据包后其再次检查我们的标记的时候又发现有要被处理的数据包了，于是其又要开始走这个流程。可能有人会想这个没关系，其它进程需要时间的时候反正你softirq是可以被中断的，那么哪怕你这里是个死循环我其它进程中断了其它进程也能正常执行，但问题是切换的时候我们的schedule会调用softirq，换句话说只要我们的标记位一直存在那么我们的softirq的处理函数会一直占有我们CPU时间，而不给其它进程执行的机会。所以最简单的方法就是让softirq在单次执行的时候其检查标记位的次数有个上限，当超过这个上限的时候就让出CPU时间，然后其余进程就能执行了。但当中断次数非常高的时候还是会存在问题，比如我下次切换进程的时候又得进入softirq的处理流程了，然后又是要等到其执行到上限后才能切换到下个进程，这会严重影响系统的响应时间。所以内核提供了ksoftirqd这个内核线程来解决这个问题。

ksoftirqd的处理逻辑很简单，其也是调用了softirq的入口函数，这个函数和我们上面举例中说的schedule中的一样，所以其会执行通常的softirq处理逻辑。当出现我们上面说的中断过多造成其它进程得不到CPU时间片的时候，softirq的处理函数会将任务交给ksoftirqd内核线程来完成。由于我们上面讲了内核通过锁等方法保证一个核只能运行一个softirq处理函数，所以当ksoftirqd开始运行处理函数的时候，在类似schedule等函数去调用softirq处理函数的时候其会发现有别的线程在处理了，所以这里就不会进入softirq的处理逻辑，这样也就能保证下一个进程可以及时被调度。另外ksoftirqd的内核线程优先级很低，在系统繁忙的时候内核就能保证其它进程的优先工作，只有在内核空闲的时候才开始调用ksoftirqd（但这并不是因为这ksoftirqd不重要，如果我一个数据包来不及接收，其它进程也相应的会从运行状态转为等待，让内核空闲出来，此时ksoftirqd就能运行了）。所以如果我们的主机收发数据包量很大的时候，我们经常能通过top等命令看到ksoftirqd占用了大量的CPU时间，同时也能观察到top命令的si这一项很高。一般对于这种情况的解决方法是看一下si或中断次数是不是集中在单核，如果是单核的话看下网卡支不支持多队列，如果支持则尝试绑定网卡队列和核，让它们不集中在一个核，如果不支持则查看内核是否支持RPS，RPS可以看成是软件实现的多队列。如果此时CPU使用率还是非常高（并且通过sar等命令确定是网络引起的），那么要判定下是不是正常的流量，否则可能就是被攻击了，对于攻击则就要采用攻击对应的应急方案，或者通过自动化的处理脚本自动修复。

对于softirq我们就讲这么多，需要大家记住的其实就两点：内核将耗时但不是必要的工作在下半区中执行而不是在中断上下文中执行，另外每个CPU核拥有一个softnet_data结构体保存每个CPU的独有信息，而最基本的收包方式中，我们的待处理sk_buff就绑在该结构下面。


### proc文件系统

proc文件系统（以及/proc/sys）是一个我们可以通过普通操作文件的方法（cat/echo）来获取或改变内核变量的一个从用户态和内核态进行交互的机制。我们在之前提过，内核通过module机制实现了一个分层的松耦合的架构，proc可以看成是虚拟文件系统这一层的一个非常有用的例子。首先先说下什么是文件系统，我们都知道硬盘中可以存放我们的文件，当我们要访问这些文件的时候我们需要一个机制告诉我们我们的文件bingotree.txt存放在硬盘的哪个地方。文件系统最简单的功能就是存放了我们逻辑上看到的物理文件以及其真实的在磁盘上存储的扇区信息的映射关系的一个东西。最简单的，其提供了read和write两个函数，当我们调用如read("bingotree.txt")的时候，文件系统就会根据文件名（或者更贴切的说文件路径）找到bingotree.txt这个文件对应的扇区信息，然后通过IO子系统读取硬盘上的信息到内存，然后read函数会返回一个指向这块内存的指针。write函数则类似，只不过这里是通过IO子系统写入对应的内容到磁盘上。

从上面的表述可以看出，我们的文件系统只需要提供类似于read或write等少数的基础方法即可。对于我们这么一个支持module机制的内核，我们完全可以把文件系统这一块做个分层，提供一个基类，基类拥有read和write方法，但是具体的实现则由加载的模块实现。所以我们会看到在linux中存在很多不同格式的文件系统，比如ext3、ext4等。在linux的世界，这种文件系统的松耦合的实现方式被叫做虚拟文件系统。

既然虚拟文件系统这么灵活，所以内核实现了proc这么一个文件系统（也就是一个proc的module），它的read和write等方法有点特别，当我调用read("/proc/uptime")的时候，proc的module的read方法并不会将/proc/uptime翻译成磁盘上的扇区信息，而是会去读取内核中的uptime信息获取系统启动了多少时间。通过这种方法，我们可以通过cat或echo这种命令获取或改变内核中的变量。

当然上面说的比较简单，但是其功能基本上就是这样，有兴趣的同学可以深入学习下。对于我们这本基础的书籍来说，大家只要知道虚拟文件系统是啥，以及proc文件系统的读写本质上是转换到内核变量的读写即可。


### 总结

本节主要是为本章下面的小节做准备工作。希望大家能记住的是：

* sk_buff代表一个数据包
* net_device代表一个网络设备
* 硬件中断是外部设备告知事件发生的一种机制，大家需要知道中断向量号及处理函数有什么用
* softirq的简单原理，ksoftirqd内核线程的作用以及softnet_data是每个核独立拥有的
* 虚拟文件系统以及proc文件系统的作用








