## 内核网络相关基础知识

这一章主要是为了后面的章节做准备的，内容包括：

- sk_buff数据结构
- net_device数据结构
- 硬件中断
- softirq
- proc文件系统

关于这些方面的小秦所发现的最好的参考资料是《Understanding Linux Network Internals》。强烈建议每个对Linux内核中网络有兴趣的同学读下这本书。本书中与底层网络相关的资料都是参考着这本书而来的。

### sk_buff数据结构

sk_buff在内核的地位简单的说就是：它代表了一个数据包。

首先来个大致的介绍让大家感受下它的重要性吧。比如一个网卡收到了一个包，此时大家可以认为这个数据包对于网卡来说只是一段电信号或光信号，网卡收到这个包后，会先保存在网卡本地，然后会告诉内核说：“内核君，我这有个包，快来拿”。内核于是就会去取这个包，既然是内核去取，那么这个包肯定到了内核后就要放到内存中的某个数据结构里了，这个数据结果对于内核来说就是sk_buff。所以我们可以认为此时这个sk_buff数据结构包含了这个数据包的所有内容。内核得到了这个sk_buff后，接下来做的事情就是让它走协议栈，我们就把协议栈当成是一个有n层楼的大厦、sk_buff是一个携带了传单想要在楼顶散发传单的小伙好了。于是sk_buff这个小伙就开始爬大厦，可能它能一直爬过所有楼层爬到楼顶，也可能在某一层就因为迷路或者找错大厦之类的原因被原路返回（也就是要下楼了）或者因为非法发传单而被捕了。在它爬楼的过程中每一层楼的楼管都会对其要发的传单做些标注或修改，但不论怎样传单在小伙爬到楼顶散发前都是属于这个小伙的，楼管不管做什么操作也只是在这个小伙的传单上做操作，如果某层的某个居民想了解传单的内容，那么必须去找这个小伙了解。所以可以看到，sk_buff就是我们的数据包在从数据包到达内核以及这个包最终到达上层之间唯一拥有这个数据包内容的结构体。其包含了网卡收到的数据包的全部信息以及协议栈对这个数据包做出的相关附加信息。

下面来看下这个sk_buff结构体的几个重要属性，列出的属性中有些属性在后面的章节会成为主角：

```
struct sk_buff {
	......
	struct sk_buff *next; //指向下一个sk_buff
	struct sk_buff *prev; //指向前一个sk_buff
	
	struct sock *sk; //拥有这个sk_buff的socket
	
	struct net_device *dev; //发送或者接收这个数据包的设备
	
	unsigned int len; //数据包的真实长度
	
	unsigned char *head;	unsigned char *end;	unsigned char *data;	unsigned char *tail;
	
	__u32 priority;  //数据包的优先级
	
	__u8 pkt_type:3; //数据包的类型
	__be16 protocol; //驱动程序判定的这个数据包的协议类型
	......
}
```

一开始的例子中举例了小伙发传单爬大楼的例子，sk_buff在内核协议栈中就是这样一层一层走协议栈的。第一层可能是链路层（L2），然后爬到IP层（L3），再然后会爬到传输层（L4）。在走协议栈的过程中，sk_buff中的一些属性可能会被修改。先来说下上面列出的几个重要属性。

next/prev:
    
	内核中的所有sk_buff都被串在了一个双向链表里以便检索和管理。这两个指针分别指向双向链表的前者和后者。
    
sk:

	如果一个数据包是发给本机的或者是由本机生成的，那么一般会有上层应用接收或产生这个数据包。写过网络代码的人都知道目前Linux是通过socket来实现网络编程的，这个sk指针就指向了将要接收这个数据包或者是产生了这个数据包的socket。如果这个数据包是走转发路径的呢（比如我本机用来做路由器）？此时这个指针就是NULL。
	
dev:

	我们下面一节就会讲到这个结构体。net_device在内核中代表了一个网络设备，比如我们的eth0、lo设备等。在sk_buff中这个结构体指针指的是发送或接收这个数据包的设备。在我们下面的章节中，net_device这个结构体要远比sk_buff重要的多。
	
pkt_type:

	这个值是根据sk_buff的二层目的地址判定出的数据包的类型。所有的类型可以在include/linux/if_packet.h中找到。
	
protocol:

	protocol指的是三层的协议类型。一般是IP、IPv6以及ARP。这个值一般是在收到数据包后，由对应的网卡的驱动程序判断数据包的三层协议类型后设置的。在我们之后将收包流程的时候会看到这个值是如何被驱动设置的。
	
head/end/data/tail:

	这四个指针分别指向了sk_buff中的不同的数据段（可以把sk_buff中的其它熟悉看成是元信息，data或head指向的才是我们真实的数据包内容）。简单的说：head指向了我们的sk_buff的最开头，end指向了我们的sk_buff的最末尾。data指向了我们真正协议报文内容的最开头，tail指向了我们真正协议报文内容的最末尾。为什么会这么复杂呢？原因在于内存的分配、扩容操作是一个比较耗时的操作。在学校网络课的时候网络老师最有可能举的一个协议栈的例子就是邮局送信。我们写好信后，我们得将它放到信封里，信封上贴好邮票写上地址才能投递出去。投递的过程中信连着信封会放在邮局的小卡车上在城市间传输，传送到目的地后收件人会拆开信封，然后才会看到信的内容。对于协议栈也存在这么一个加信封、解信封的过程，在协议栈里信封就是我们的协议头。在内存中如果我们明确知道要分配多大的内存，那么我们直接分配一块连续内存就行了（可以理解这个内存就是我们的信的内容），但是由于我们有增加头部、移除头部（加信封、解信封）的过程，我们势必要在我们申请的内存的头部后者尾部再次申请内存。在已有的一块内存的头部或尾部再次附加一段内存对于内核来说可是一件比较痛苦的事情，所以内核的方法很简单：一次性就申请一大块足够大的内存，这块内存的头就是head指针指向的地方，尾就是end指向的地方。但这块内存中真正有意义的数据由data和tail来标示。比如一开始这块内存只存在TCP的数据，所以data会的指向TCP的头部，此时data和head之间的一大块空闲内存就还空着。然后这个包传到了IP层，IP也要给这个数据包加上IP的头部，于是data指针会往head指针靠近，其会的靠近IP头部的长度这么一块距离。
	
len:

	len指的是我们上面说的data指针与tail指针之间的差值，也就是数据包报文目前被协议栈认为的有意义的长度的大小。因此当sk_buff在协议栈之间传递的时候，随着data或tail指针的变化，其长度也会相应变化。
	
	
关于sk_buff我们就介绍这么多，对于我们接下来的内容来说，大家最重要的是要记住sk_buff代表了我们的数据包，在内核协议栈中数据包通过sk_buff这个数据结构来在协议栈之间传递。
	

### net_device数据结构

如果我们通过ifconfig或ip link show命令查看本机的网卡，我们可以看到类似如下的输出：

```
[root@dev ~]# ip link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:89:4a:1e brd ff:ff:ff:ff:ff:ff
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:77:2b:ca brd ff:ff:ff:ff:ff:ff
4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:66:66:88 brd ff:ff:ff:ff:ff:ff
5: enp0s10: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 08:00:27:5e:84:f9 brd ff:ff:ff:ff:ff:ff
6: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT 
    link/ether 52:54:00:27:e8:bc brd ff:ff:ff:ff:ff:ff
7: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN mode DEFAULT qlen 500
    link/ether 52:54:00:27:e8:bc brd ff:ff:ff:ff:ff:ff
```

这里我们的ip命令列出了本机的所有网卡，比如lo、enp0s3、virbr0等。其中即有lo这个回环设备，也有enp0s3这个真实的物理设备，还存在virbr0这个网桥。在内核中，这些都是由net_device这个结构体来表示的。一个net_device代表了一个网络设备，包括真实的物理设备、虚拟设备或者网桥等。当一个网络设备被驱动识别后，驱动就会在内核中建立一个net_device数据结构代表这个设备，之后在数据包的发送和接收的时候，这个设备都会作为重要的参数在各个函数之间传递（因此C没有对象的概念......）。我们来看下其重要的一些属性：

```
struct net_device {
	......
	char name[IFNAMSIZ]; //设备名
	
	int irq; //这个设备注册的中断向量号
	
	unsigned int flags; //用于表示这个设备的一些说明
	
	unsigned int mtu; //表示这个设备的mtu大小
	unsigned int type; //表示这个设备支持的网络类型，比如以太网
	unsigned long state; //表示这个设备的状态
	
	struct Qdisc *qdisc; //通过tc进行数据包的发送的时候会的用到这个
	
	const struct net_device_ops *netdev_ops; //这个设备所支持的大部分操作的函数实现
	const struct ethtool_ops *ethtool_ops; //这个设备所支持的ethtool操作的函数实现
	......
}
```

我们来看下这几个属性代表了什么。

name:

	name就是我们通过ip link show看到的lo、enp0s3这样的名字。
	
irq:

	在我们下面讲中断的时候会的讲到中断号的作用。简单的理解就是：当外部硬件设备发生了一件事情的时候，会需要某个方式通知内核，而对于内核来说其只会知道目前XXX发生了事情，而XXX就是个数字。于是内核去一个类似于Python字典结构中根据XXX去找对应的XXX的处理函数，然后被动的执行这个函数去处理这个事件。XXX就是我们这里的irq，如果中断没有共享的话这个irq和我们的net_device是唯一对应的，否则驱动程序要判断产生的中断事件是不是其自己所关心的。比如三个设备都是用10这个irq号，此时内核收到了一个中断号为10的事件，于是内核会依次调用这个三个设备的对应中断处理函数，每个函数要判断下这个事件是不是自己要处理的说（简单的说就是这个事件是不是自己设备产生的）。
	
flags:

	我们通过ip link show命令的时候可以看到这个设备的一些状态信息，这些信息就是由flags这个字段标记的（当然还有其他的字段表示其它的一些信息）。比如是否是处于混杂模式等。
	
mtu:

	mtu只的是这个设备所能传输的最大报文大小。注意和传输层的最大传输大小区分开来。
	
type:

	type表示这个网络设备的类型，比如以太网设备等。
	
state:

	state表示这个设备的状态。这个在后面我们会看到这个状态字段会影响我们的数据包能否被发送。比如如果state中__LINK_STATE_XOFF标记位为true的时候这个网卡就不能发送数据包啦。
	
qdisc:

	内核中发送数据包的时候会的走一个叫做tc（traffic control）的系统。简单的说就是一个数据包要被发送的时候，先扔到qdisc中去排队，然后再从qdisc中取出一个包，这个被取出的包才是要被发送的。至于这个包是不是我扔进去的那个包就不知道了，这得开tc配置的是什么算法了。tc在我们之前的章节中有过介绍，在之后看到发包的实现的时候大家就清楚其起作用的时间了。
	
netdev_ops:

	在解释这个之前先说下C的函数指针。在面向对象的语言中或多或少存在接口（interface）的概念。比如我一个Python类Animal，其存在一个方法是run。那么我可以有一个子类Dog继承这个Animal类，然后其必然拥有run这个方法。其可以通过覆盖父类自己实现run，或者直接使用父类的run，但是总之我们知道其肯定有run这个方法。因此通过我们之前文章中讲过的stevedore等方法我们可以动态的加载对应的driver而调用不同的实现。对于C语言来说其语言级别没有直接的接口这种特性，所以其通过函数指针的方式实现类似的功能。比如我们的结构体A拥有ops这个函数指针，然后我们写个注释说这个ops指向的函数会做打印当前主机内存的操作。然后我们在linux上跑这个程序，此时可以实现一个函数show_mem其实现为调用free方法查看内存大小。然后将ops指向这个函数，然后别人调用A的ops的时候就能看到内存大小了。同样的，在windows上我们的free方法就不管用了，于是有人在windows上也实现了一个windows版本的show_mem，然后将A结构体的ops指向这个windows版本的show_mem，此时上层调用A这个结构体的程序不用做任何修改，其调用A的ops方法依然可以正常获取到windows主机的内存大小。在net_device中这种需求就特别强烈了，比如收发包这个操作在网卡层面每家厂商都不一样，但内核的代码肯定不能根据不同的厂商动态变吧，所以内核的代码是通用的，其调用类似于A的ops方法，但是每个驱动可以实现不同的ops，于是对于驱动X，内核调用ops就会执行X的实现，对于驱动Y则内核调用ops就会执行Y的实现。netdev_ops就包含了内核需要的大部分数据包在网卡层面的收取、发送的函数指针。比如：ndo_change_mtu用于修改网卡的mtu，ndo_start_xmit用于发送数据包等。
	
ethtool_ops:

	ethtool_ops也是一个包含了大量函数指针的结构体，和netdev_ops的区别在于其主要用于实现ethtool的各种功能。在这里ethtool就是我们的上层应用，通过函数指针将其和底层实现解耦，因此内核的同一套ethtool代码对于不同的net_device可以根据驱动实现相同的功能，而不用在乎底层不同设备的不同。
	
关于net_device我们在大致了解了它一下后就先告一段落，后面我们会再讲到它，在本章中它是我们的第一主角。目前我们只要知道，它代表了一个网络设备，其包含了这个网络设备的所有内核需要的通用方法的具体实现（通过函数指针）。	


