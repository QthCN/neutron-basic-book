## 协议栈中的收包/发包

本节我们会讲一下在内核中，一个数据包从网卡进入后会走什么样的路径，以及当发送一个数据包的时候这个数据包会的走什么样的路径。除此以外当我们对数据包的收发路径大概清楚后，我们会看下Neutron中用到的veth设备在内核是如何实现的，另外会讲下常见的提升收发包速率的方法。


### PCIe设备驱动的加载

在前面一节中我们提到了内核的module机制以及硬件中断的处理函数，另外也介绍了net_device数据结构。这里我们来把这几个东西串起来。

当一台物理机上插了块网卡的时候，网卡现在都是走PCIe口插在主板上的。假设现在内核中没有任何的网卡驱动，所以当开机的时候内核会让PCIe子系统自己去发现连在上面的设备，于是PCIe子系统发现了这个网卡。PCIe是个通用的标准，虽然此时PCIe不知道如何去使用这个设备，但是通过PCIe的标准PCIe子系统可以询问这个网卡设备获取这个网卡设备的基本信息，在这个信息里有一个很重要的ID，这个ID是全球唯一的，只属于这个网卡的型号。然后PCIe虽然不知道怎么使用这个设备，但是既然知道了其ID，所以就在某个内核中存了个信息，内容类似为：“我这里有个ID位XXX的设备。”。接着我们来加载驱动，加载的方法就是之前讲的module机制，通过insmod的方法我们的驱动就加载了，然后对应的init方法会调用。由于这个驱动知道自己的网卡是个PCIe设备，所以其init方法里会直接调用内核提供的pci_module_init函数。pci_module_init函数是内核提供的，其实现在目前我们可以简单的理解为：它会去查看PCIe发现的设备列表，遍历所有设备，然后判断被遍历的设备的ID是不是和驱动代码里写的ID一样，如果这两个ID是一样的，那么这个设备就有对应的驱动了。一个实际的例子如下：

```
static int __init e100_init_module(void)
{
    return pci_module_init(&e100_driver);
}

static struct pci_driver e100_driver = {
     .name = DRV_NAME,
     .id_table = e100_id_table,
     .probe = e100_probe,
     .remove = __devexit_p(e100_remove),
#ifdef CONFIG_PM
     .suspend = e100_suspend,
     .resume = e100_resume,
#endif
};
```

我们的module的init方法为e100_init_module，其直接调用了pci_module_init去匹配对应的设备。e100_driver是我们驱动实现的，注意这里的id_table属性就是我们的PCIe的每个设备型号全球唯一的ID。

现在我们知道了设备和驱动是如何联系起来的，接着我们来看我们的net_device如何被建立。这里还是要回到pci_module_init上，我们可以看到我们的驱动实现了一个pci_driver结构体，对于每个PCIe的驱动，都需要实现这个结构体的方法。内核约定在pci_module_init被调用的时候，会调用pci_driver结构体的probe方法，我们可以将probe看成是我们驱动和设备关联后执行的一个初始化方法。至于probe中做什么事情就有PCIe的驱动自己实现了。对于网卡驱动来说一般会做下面的事情：

* 生成net_device结构体并注册到全局链表上
* 驱动会设置net_device的相关属性，尤其是提供各种ops属性的实现

我们来详细看下probe，这里来看一个实际的例子e1000，在内核源码中的路径在drivers/net/ethernet/intel/e1000下。

首先来看其pci_driver结构体：

```
static struct pci_driver e1000_driver = {
    .name     = e1000_driver_name,
    .id_table = e1000_pci_tbl,
    .probe    = e1000_probe,
    .remove   = e1000_remove,
#ifdef CONFIG_PM
    /* Power Management Hooks */
    .suspend  = e1000_suspend,
    .resume   = e1000_resume,
#endif
    .shutdown = e1000_shutdown,
    .err_handler = &e1000_err_handler
};
```

对应的probe方法我们看到是e1000_probe，我们来看下其实现。代码为：

```
static int e1000_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
{
    struct net_device *netdev;
    struct e1000_adapter *adapter;
    struct e1000_hw *hw;
 
    static int cards_found = 0;
    static int global_quad_port_a = 0; /* global ksp3 port a indication */
    int i, err, pci_using_dac;
    u16 eeprom_data = 0;
    u16 tmp = 0;
    u16 eeprom_apme_mask = E1000_EEPROM_APME;
    int bars, need_ioport;
```

这里声明及初始化了一些变量，接着的代码为：

```
/* do not allocate ioport bars when not needed */
need_ioport = e1000_is_need_ioport(pdev);
if (need_ioport) {
    bars = pci_select_bars(pdev, IORESOURCE_MEM | IORESOURCE_IO);
    err = pci_enable_device(pdev);
} else {
    bars = pci_select_bars(pdev, IORESOURCE_MEM);
    err = pci_enable_device_mem(pdev);
}
if (err)
    return err;
 
err = pci_request_selected_regions(pdev, bars, e1000_driver_name);
if (err)
    goto err_pci_reg;
 
pci_set_master(pdev);
err = pci_save_state(pdev);
if (err)
    goto err_alloc_etherdev;
```

这些代码和cpi有关，但是不是我们的重点。我们继续看代码：

```
err = -ENOMEM;
netdev = alloc_etherdev(sizeof(struct e1000_adapter));
if (!netdev)
    goto err_alloc_etherdev;
```

可以看到这里调用了内核提供的alloc_etherdev分配了一个以太网的net_device结构体。alloc_etherdev实际上是alloc_netdev的一个封装，alloc_netdev会分配已个最基本的net_device结构，然后alloc_etherdev在调用alloc_netdev的基础上会对这个net_device设置一些以太网相关的属性，这些以太网相关属性的初始化是通过调用ether_setup实现的。另外我们可以看到alloc_etherdev传递了一个e1000_adapter结构体的大小，这个大小是分配net_device的时候用于决定私有数据大小的。现在我们的net_device已经出现，只不过是一个通用的以太网的net_device结构体。我们继续看代码：

```
SET_NETDEV_DEV(netdev, &pdev->dev);

pci_set_drvdata(pdev, netdev);
adapter = netdev_priv(netdev);
adapter->netdev = netdev;
adapter->pdev = pdev;
adapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);
adapter->bars = bars;
adapter->need_ioport = need_ioport;
 
hw = &adapter->hw;
hw->back = adapter;
```

这里的代码主要是私有数据结构的初始化，私有数据结构所在的内存大小就是上面说的alloc_etherdev传递e1000_adapter结构体的大小。私有的我们就不去细看了，大家可以注意下netdev_priv，在大家自己去看网络相关的代码的时候看到netdev_priv就知道返回的指针是指向net_device的私有数据内存的指针。继续看代码，我们跳过那些私有的代码：

```
netdev->netdev_ops = &e1000_netdev_ops;
e1000_set_ethtool_ops(netdev);
netdev->watchdog_timeo = 5 * HZ;
netif_napi_add(netdev, &adapter->napi, e1000_clean, 64);
 
strncpy(netdev->name, pci_name(pdev), sizeof(netdev->name) - 1);
```

我们可以看到我们的netdev_ops和ethtool_ops都在这里被驱动给设置了。关于napi相关的我们后面会详细讲到。e1000_netdev_ops的内容为：

```
static const struct net_device_ops e1000_netdev_ops = {
    .ndo_open       = e1000_open,
    .ndo_stop       = e1000_close,
    .ndo_start_xmit     = e1000_xmit_frame,
    .ndo_get_stats      = e1000_get_stats,
    .ndo_set_rx_mode    = e1000_set_rx_mode,
    .ndo_set_mac_address    = e1000_set_mac,
    .ndo_tx_timeout     = e1000_tx_timeout,
    .ndo_change_mtu     = e1000_change_mtu,
    .ndo_do_ioctl       = e1000_ioctl,
    .ndo_validate_addr  = eth_validate_addr,
    .ndo_vlan_rx_add_vid    = e1000_vlan_rx_add_vid,
    .ndo_vlan_rx_kill_vid   = e1000_vlan_rx_kill_vid,
#ifdef CONFIG_NET_POLL_CONTROLLER
    .ndo_poll_controller    = e1000_netpoll,
#endif
    .ndo_fix_features   = e1000_fix_features,
    .ndo_set_features   = e1000_set_features,
};
```

这里的一些方法（比如ndo_start_xmit）会在后面的小节中讲到。接着看我们的probe方法可以看到：

```
strcpy(netdev->name, "eth%d");
err = register_netdev(netdev);
if (err)
    goto err_register;
```

这里可以看到我们的设备名字被设置了，重点可以看到register_netdev，其将我们的net_device结构体加入了全局的链表中，此时我们调用ip link show的时候后者就会遍历全局的链表输出我们的设备了。

probe的代码在e1000中就基本上这些，这里可能有人会有疑问，我们的net_device已经建立好了，但是我们的中断处理函数和中断向量号的绑定是在哪里设置的呢？目前代码看下来我们的probe并没有做这个事情，此时如果网卡收到了包产生了中断号，但由于这个中断号并没有对应的处理函数所以内核是收不到包的。在e1000的代码中，中断向量号和中断处理函数的绑定是在netdev_ops的ndo_open方法中设置的，在e1000_open中我们可以看到如下代码：

```
static int e1000_request_irq(struct e1000_adapter *adapter)
{
    struct net_device *netdev = adapter->netdev;
    irq_handler_t handler = e1000_intr;
    int irq_flags = IRQF_SHARED;
    int err;
 
    err = request_irq(adapter->pdev->irq, handler, irq_flags, netdev->name,
                      netdev);
    if (err) {
        e_err(probe, "Unable to allocate interrupt Error: %d\n", err);
    }   
 
    return err;
}
```

request_irq这里设置了adapter->pdev->irq这个中断向量号和中断handler的关系。另外通过上面的代码我们也清楚了，当我们想要学习一个驱动的收包代码的时候，e1000_intr会是我们的一个很好的入口：）。

关于PCIe的网卡驱动加载我们就讲这么多，重要的是要记住net_device是如何生成的以及其在哪里进行了ops的函数指针的赋值。


### 收包路径

现在我们来讲收包。在本节和本书下面的章节中我们会看到数据包如何从网卡进入内核，然后开始走协议栈一层一层往上走。我们不会讲高层协议栈中对于收包的处理，因为对于我们学习Neutron来说那个是应用的事情了，但是我想如果能理解下面讲的基础收包路径，那么大家很容易能继续探索协议栈上层收包的实现的。本节主要讲一个数据包是如何从网卡到内核并开始走协议栈的这么一个流程。

在前面的小节中我们已经看到了net_device这个结构体的初始化，下面我们就以eth0这个网卡为例子来看下其收包的逻辑。eth0的网卡插在PCIe卡上，然后根据上节说的内容，根据eth0所在网卡型号的全球唯一ID，驱动程序会认领这个设备，然后初始化net_device的属性。另外在我们上面e1000的例子中，当网卡open的时候其会绑定中断向量号和对应的中断处理函数e1000_intr。

内核中收包的数据路径目前有两种主流的，一个是基本的收包路径，简单的说就是一个包到达后先把这个包放到一个队列里，然后下半区的softirq handler会来处理，另一个是NAPI（New API，这名字有点俗...），后者可以通过poll这个轮询的方法处理数据包。这两种路径小秦下面都会讲到。

首先我们来讲NAPI，为什么现讲NAPI而不先讲基础的收包流程呢？因为目前内核中基础的收包流程是基于NAPI的流程实现的。

我们讲PCIe设备驱动加载的时候说到过probe方法会的进行net_device的初始化，net_device有一个间接关联的重要属性叫做poll，其也是一个函数指针。这个poll的实际实现由驱动实现，其作用是说内核可以通过调用poll直接从网卡上获取一定的数据包。这里可以看出poll其实是类似轮询的方法，由内核主动去向网卡获取数据包而不是由网卡通过中断汇报数据包给内核。

现在想象下一个数据包到达了网卡，然后网卡会触发硬件中断，硬件中断打断了CPU的当前执行上下文转到了内核中处理中断相关的代码，然后这段内核代码根据中断号找到中断处理函数，接着调用这个函数。这个中断处理函数根据我们上面小节所讲的是由驱动提供的。对于NAPI的驱动来说，这里中断处理函数做的事情主要有两个：第一是将当前的net_device结构体加入到我们在5.1小节将的每个CPU核独有的softnet_data结构体的poll_list链表上，第二是设置下收包的softirq标记NET_RX_SOFTIRQ。然后中断处理函数就结束了（可以看到这个中断函数没有做太多是事情）。

接着softirq上场了，我们已经知道softirq和硬件中断类似，都会根据某个标识去查找对应的处理函数。内核中NET_RX_SOFTIRQ的处理函数为net_rx_action。

net_rx_action的逻辑是这样的，其会遍历softnet_data的poll_list，对于poll_list上的每个net_device设备调用其poll函数从网卡上获取数据包。拿到数据包后就开始走协议栈了，怎么样才能让一个数据包走内核的协议栈呢？poll函数中可以调用内核提供的netif_receive_skb，并传递sk_buff，netif_receive_skb会的让这个数据包走协议栈。

上面已经看到了NAPI是怎么让数据包从网卡进入到走协议栈的一个简单流程，当然有很多细节目前是略过的，比如对于poll的调用每次是只能获取一定数量的数据包的，否则一个网卡上如果不停的有数据包进来那么整个系统会卡在这个网卡上。所以可以近似的认为每次只能获取n个数据包，如果获取完n个后还有数据包可以获取，那么会先将这个net_device移动到poll_list的末尾，然后开始处理下一个poll_list上的net_device。不过基本的流程就是上面讲的这样。

现在来看下非NAPI的收包流程，也就是基本的收包流程。想象一下一个数据包到达了网卡，然后网卡会触发硬件中断，接着中断处理函数被调用。在非NAPI的驱动程序中，内核规范了下面的执行流程：驱动负责从网卡拷贝一个数据包到内核的一个sk_buff中，然后中断调用内核提供的netif_rx方法让这个数据包开始走收包的流程。在以前没有NAPI的时候netif_rx做的事情就是把这个sk_buff放到softnet_data的nput_pkt_queue链表中，然后设置softirq标记等待处理函数来处理。但是现在的内核的netif_rx的逻辑已经改了，现在的netif_rx会的建立一个此网卡net_device结构体对应的类似的net_device结构体（比如名字叫做net_device_2），然后netif_rx会把这个net_device_2放到softnet_data的poll_list上，同时和以前的实现一样，会从网卡获取数据包到sk_buff并将其放到softnet_data的nput_pkt_queue链表中，然后设置softirq标记等待处理函数来处理。通过上面的NAPI的流程学习我们知道这里的处理函数net_rx_action会遍历poll_list，然后一次调用遍历得到的net_device的poll方法。于是我们的net_device_2的poll方法被调用。此时这个poll方法就是由内核实现的了，一般是绑定到内核提供的process_backlog函数上。这个函数做的事情就是从softnet_data的nput_pkt_queue链表中获取一个sk_buff，然后调用上面讲到的netif_receive_skb让这个数据包开始走协议栈。可以看到通过修改netif_rx的实现对于老的驱动代码不需要改动就能共用同一套NAPI的基础架构。

那么NAPI有什么好处呢？对于Neutron网络节点来说，在非DVR这种情况下列如外网口的这类网卡，其数据包的个数经常是很大的，数据包量大了后中断的方式效率就太低下了，所以通过NAPI的poll方法可以减少网卡的负担，由CPU主动根据情况去网卡上获取数据包。另外NAPI的驱动也可以实现的很灵活，可以在中断和轮询这两者间切换，比如一个新的数据包到达，然后发现内核很繁忙且还有未处理的数据包的时候，其会将中断模式改为poll，于是新的数据包到达后就不会打断内核，同时当内核空闲的时候其会去轮询设备，查看是否有要处理的数据包。当压力下来后又可以恢复中断模式。通过这种方法在有大量的数据包环境中可以减少CPU使用率，并在只有少量数据包的时候降低响应时间。

现在我们的数据包sk_buff已经站在内核协议栈的门口了，通过netif_receive_skb其会开始内核协议栈之旅，对于netif_receive_skb在我们后面的章节中会讲到其实现。



### 发包路径


