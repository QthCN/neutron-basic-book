## 协议栈中的收包/发包

本节我们会讲一下在内核中，一个数据包从网卡进入后会走什么样的路径，以及当发送一个数据包的时候这个数据包会的走什么样的路径。除此以外当我们对数据包的收发路径大概清楚后，我们会看下Neutron中用到的veth设备在内核是如何实现的，另外会讲下常见的提升收发包速率的方法。


### PCIe设备驱动的加载

在前面一节中我们提到了内核的module机制以及硬件中断的处理函数，另外也介绍了net_device数据结构。这里我们来把这几个东西串起来。

当一台物理机上插了块网卡的时候，网卡现在都是走PCIe口插在主板上的。假设现在内核中没有任何的网卡驱动，所以当开机的时候内核会让PCIe子系统自己去发现连在上面的设备，于是PCIe子系统发现了这个网卡。PCIe是个通用的标准，虽然此时PCIe不知道如何去使用这个设备，但是通过PCIe的标准PCIe子系统可以询问这个网卡设备获取这个网卡设备的基本信息，在这个信息里有一个很重要的ID，这个ID是全球唯一的，只属于这个网卡的型号。然后PCIe虽然不知道怎么使用这个设备，但是既然知道了其ID，所以就在某个内核中存了个信息，内容类似为：“我这里有个ID位XXX的设备。”。接着我们来加载驱动，加载的方法就是之前讲的module机制，通过insmod的方法我们的驱动就加载了，然后对应的init方法会调用。由于这个驱动知道自己的网卡是个PCIe设备，所以其init方法里会直接调用内核提供的pci_module_init函数。pci_module_init函数是内核提供的，其实现在目前我们可以简单的理解为：它会去查看PCIe发现的设备列表，遍历所有设备，然后判断被遍历的设备的ID是不是和驱动代码里写的ID一样，如果这两个ID是一样的，那么这个设备就有对应的驱动了。一个实际的例子如下：

```
static int __init e100_init_module(void)
{
    return pci_module_init(&e100_driver);
}

static struct pci_driver e100_driver = {
     .name = DRV_NAME,
     .id_table = e100_id_table,
     .probe = e100_probe,
     .remove = __devexit_p(e100_remove),
#ifdef CONFIG_PM
     .suspend = e100_suspend,
     .resume = e100_resume,
#endif
};
```

我们的module的init方法为e100_init_module，其直接调用了pci_module_init去匹配对应的设备。e100_driver是我们驱动实现的，注意这里的id_table属性就是我们的PCIe的每个设备型号全球唯一的ID。

现在我们知道了设备和驱动是如何联系起来的，接着我们来看我们的net_device如何被建立。这里还是要回到pci_module_init上，我们可以看到我们的驱动实现了一个pci_driver结构体，对于每个PCIe的驱动，都需要实现这个结构体的方法。内核约定在pci_module_init被调用的时候，会调用pci_driver结构体的probe方法，我们可以将probe看成是我们驱动和设备关联后执行的一个初始化方法。至于probe中做什么事情就有PCIe的驱动自己实现了。对于网卡驱动来说一般会做下面的事情：

* 生成net_device结构体并注册到全局链表上
* 驱动会设置net_device的相关属性，尤其是提供各种ops属性的实现

我们来详细看下probe，这里来看一个实际的例子e1000，在内核源码中的路径在drivers/net/ethernet/intel/e1000下。

首先来看其pci_driver结构体：

```
static struct pci_driver e1000_driver = {
    .name     = e1000_driver_name,
    .id_table = e1000_pci_tbl,
    .probe    = e1000_probe,
    .remove   = e1000_remove,
#ifdef CONFIG_PM
    /* Power Management Hooks */
    .suspend  = e1000_suspend,
    .resume   = e1000_resume,
#endif
    .shutdown = e1000_shutdown,
    .err_handler = &e1000_err_handler
};
```

对应的probe方法我们看到是e1000_probe，我们来看下其实现。代码为：

```
static int e1000_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
{
    struct net_device *netdev;
    struct e1000_adapter *adapter;
    struct e1000_hw *hw;
 
    static int cards_found = 0;
    static int global_quad_port_a = 0; /* global ksp3 port a indication */
    int i, err, pci_using_dac;
    u16 eeprom_data = 0;
    u16 tmp = 0;
    u16 eeprom_apme_mask = E1000_EEPROM_APME;
    int bars, need_ioport;
```

这里声明及初始化了一些变量，接着的代码为：

```
/* do not allocate ioport bars when not needed */
need_ioport = e1000_is_need_ioport(pdev);
if (need_ioport) {
    bars = pci_select_bars(pdev, IORESOURCE_MEM | IORESOURCE_IO);
    err = pci_enable_device(pdev);
} else {
    bars = pci_select_bars(pdev, IORESOURCE_MEM);
    err = pci_enable_device_mem(pdev);
}
if (err)
    return err;
 
err = pci_request_selected_regions(pdev, bars, e1000_driver_name);
if (err)
    goto err_pci_reg;
 
pci_set_master(pdev);
err = pci_save_state(pdev);
if (err)
    goto err_alloc_etherdev;
```

这些代码和cpi有关，但是不是我们的重点。我们继续看代码：

```
err = -ENOMEM;
netdev = alloc_etherdev(sizeof(struct e1000_adapter));
if (!netdev)
    goto err_alloc_etherdev;
```

可以看到这里调用了内核提供的alloc_etherdev分配了一个以太网的net_device结构体。alloc_etherdev实际上是alloc_netdev的一个封装，alloc_netdev会分配已个最基本的net_device结构，然后alloc_etherdev在调用alloc_netdev的基础上会对这个net_device设置一些以太网相关的属性，这些以太网相关属性的初始化是通过调用ether_setup实现的。另外我们可以看到alloc_etherdev传递了一个e1000_adapter结构体的大小，这个大小是分配net_device的时候用于决定私有数据大小的。现在我们的net_device已经出现，只不过是一个通用的以太网的net_device结构体。我们继续看代码：

```
SET_NETDEV_DEV(netdev, &pdev->dev);

pci_set_drvdata(pdev, netdev);
adapter = netdev_priv(netdev);
adapter->netdev = netdev;
adapter->pdev = pdev;
adapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);
adapter->bars = bars;
adapter->need_ioport = need_ioport;
 
hw = &adapter->hw;
hw->back = adapter;
```

这里的代码主要是私有数据结构的初始化，私有数据结构所在的内存大小就是上面说的alloc_etherdev传递e1000_adapter结构体的大小。私有的我们就不去细看了，大家可以注意下netdev_priv，在大家自己去看网络相关的代码的时候看到netdev_priv就知道返回的指针是指向net_device的私有数据内存的指针。继续看代码，我们跳过那些私有的代码：

```
netdev->netdev_ops = &e1000_netdev_ops;
e1000_set_ethtool_ops(netdev);
netdev->watchdog_timeo = 5 * HZ;
netif_napi_add(netdev, &adapter->napi, e1000_clean, 64);
 
strncpy(netdev->name, pci_name(pdev), sizeof(netdev->name) - 1);
```

我们可以看到我们的netdev_ops和ethtool_ops都在这里被驱动给设置了。关于napi相关的我们后面会详细讲到。e1000_netdev_ops的内容为：

```
static const struct net_device_ops e1000_netdev_ops = {
    .ndo_open       = e1000_open,
    .ndo_stop       = e1000_close,
    .ndo_start_xmit     = e1000_xmit_frame,
    .ndo_get_stats      = e1000_get_stats,
    .ndo_set_rx_mode    = e1000_set_rx_mode,
    .ndo_set_mac_address    = e1000_set_mac,
    .ndo_tx_timeout     = e1000_tx_timeout,
    .ndo_change_mtu     = e1000_change_mtu,
    .ndo_do_ioctl       = e1000_ioctl,
    .ndo_validate_addr  = eth_validate_addr,
    .ndo_vlan_rx_add_vid    = e1000_vlan_rx_add_vid,
    .ndo_vlan_rx_kill_vid   = e1000_vlan_rx_kill_vid,
#ifdef CONFIG_NET_POLL_CONTROLLER
    .ndo_poll_controller    = e1000_netpoll,
#endif
    .ndo_fix_features   = e1000_fix_features,
    .ndo_set_features   = e1000_set_features,
};
```

这里的一些方法（比如ndo_start_xmit）会在后面的小节中讲到。接着看我们的probe方法可以看到：

```
strcpy(netdev->name, "eth%d");
err = register_netdev(netdev);
if (err)
    goto err_register;
```

这里可以看到我们的设备名字被设置了，重点可以看到register_netdev，其将我们的net_device结构体加入了全局的链表中，此时我们调用ip link show的时候后者就会遍历全局的链表输出我们的设备了。

probe的代码在e1000中就基本上这些，这里可能有人会有疑问，我们的net_device已经建立好了，但是我们的中断处理函数和中断向量号的绑定是在哪里设置的呢？目前代码看下来我们的probe并没有做这个事情，此时如果网卡收到了包产生了中断号，但由于这个中断号并没有对应的处理函数所以内核是收不到包的。在e1000的代码中，中断向量号和中断处理函数的绑定是在netdev_ops的ndo_open方法中设置的，在e1000_open中我们可以看到如下代码：

```
static int e1000_request_irq(struct e1000_adapter *adapter)
{
    struct net_device *netdev = adapter->netdev;
    irq_handler_t handler = e1000_intr;
    int irq_flags = IRQF_SHARED;
    int err;
 
    err = request_irq(adapter->pdev->irq, handler, irq_flags, netdev->name,
                      netdev);
    if (err) {
        e_err(probe, "Unable to allocate interrupt Error: %d\n", err);
    }   
 
    return err;
}
```

request_irq这里设置了adapter->pdev->irq这个中断向量号和中断handler的关系。另外通过上面的代码我们也清楚了，当我们想要学习一个驱动的收包代码的时候，e1000_intr会是我们的一个很好的入口：）。

关于PCIe的网卡驱动加载我们就讲这么多，重要的是要记住net_device是如何生成的以及其在哪里进行了ops的函数指针的赋值。


### 收包路径

现在我们来讲收包。在本节和本书下面的章节中我们会看到数据包如何从网卡进入内核，然后开始走协议栈一层一层往上走。我们不会讲高层协议栈中对于收包的处理，因为对于我们学习Neutron来说那个是应用的事情了，但是我想如果能理解下面讲的基础收包路径，那么大家很容易能继续探索协议栈上层收包的实现的。本节主要讲一个数据包是如何从网卡到内核并开始走协议栈的这么一个流程。

在前面的小节中我们已经看到了net_device这个结构体的初始化，下面我们就以eth0这个网卡为例子来看下其收包的逻辑。eth0的网卡插在PCIe卡上，然后根据上节说的内容，根据eth0所在网卡型号的全球唯一ID，驱动程序会认领这个设备，然后初始化net_device的属性。另外在我们上面e1000的例子中，当网卡open的时候其会绑定中断向量号和对应的中断处理函数e1000_intr。

内核中收包的数据路径目前有两种主流的，一个是基本的收包路径，简单的说就是一个包到达后先把这个包放到一个队列里，然后下半区的softirq handler会来处理，另一个是NAPI（New API，这名字有点俗...），后者可以通过poll这个轮询的方法处理数据包。这两种路径小秦下面都会讲到。

首先我们来讲NAPI，为什么现讲NAPI而不先讲基础的收包流程呢？因为目前内核中基础的收包流程是基于NAPI的流程实现的。

我们讲PCIe设备驱动加载的时候说到过probe方法会的进行net_device的初始化，net_device有一个间接关联的重要属性叫做poll，其也是一个函数指针。这个poll的实际实现由驱动实现，其作用是说内核可以通过调用poll直接从网卡上获取一定的数据包。这里可以看出poll其实是类似轮询的方法，由内核主动去向网卡获取数据包而不是由网卡通过中断汇报数据包给内核。

现在想象下一个数据包到达了网卡，然后网卡会触发硬件中断，硬件中断打断了CPU的当前执行上下文转到了内核中处理中断相关的代码，然后这段内核代码根据中断号找到中断处理函数，接着调用这个函数。这个中断处理函数根据我们上面小节所讲的是由驱动提供的。对于NAPI的驱动来说，这里中断处理函数做的事情主要有两个：第一是将当前的net_device结构体加入到我们在5.1小节将的每个CPU核独有的softnet_data结构体的poll_list链表上，第二是设置下收包的softirq标记NET_RX_SOFTIRQ。然后中断处理函数就结束了（可以看到这个中断函数没有做太多是事情）。

接着softirq上场了，我们已经知道softirq和硬件中断类似，都会根据某个标识去查找对应的处理函数。内核中NET_RX_SOFTIRQ的处理函数为net_rx_action。

net_rx_action的逻辑是这样的，其会遍历softnet_data的poll_list，对于poll_list上的每个net_device设备调用其poll函数从网卡上获取数据包。拿到数据包后就开始走协议栈了，怎么样才能让一个数据包走内核的协议栈呢？poll函数中可以调用内核提供的netif_receive_skb，并传递sk_buff，netif_receive_skb会的让这个数据包走协议栈。

上面已经看到了NAPI是怎么让数据包从网卡进入到走协议栈的一个简单流程，当然有很多细节目前是略过的，比如对于poll的调用每次是只能获取一定数量的数据包的，否则一个网卡上如果不停的有数据包进来那么整个系统会卡在这个网卡上。所以可以近似的认为每次只能获取n个数据包，如果获取完n个后还有数据包可以获取，那么会先将这个net_device移动到poll_list的末尾，然后开始处理下一个poll_list上的net_device。还有就是NAPI其实是在系统压力较大的时候才会开始poll，平时都是正常中断响应数据包的。不过基本的流程就是上面讲的这样。

现在来看下非NAPI的收包流程，也就是基本的收包流程。想象一下一个数据包到达了网卡，然后网卡会触发硬件中断，接着中断处理函数被调用。在非NAPI的驱动程序中，内核规范了下面的执行流程：驱动负责从网卡拷贝一个数据包到内核的一个sk_buff中，然后中断调用内核提供的netif_rx方法让这个数据包开始走收包的流程。在以前没有NAPI的时候netif_rx做的事情就是把这个sk_buff放到softnet_data的nput_pkt_queue链表中，然后设置softirq标记等待处理函数来处理。但是现在的内核的netif_rx的逻辑已经改了，现在的netif_rx会的建立一个此网卡net_device结构体对应的类似的net_device结构体（比如名字叫做net_device_2），然后netif_rx会把这个net_device_2放到softnet_data的poll_list上，同时和以前的实现一样，会从网卡获取数据包到sk_buff并将其放到softnet_data的nput_pkt_queue链表中，然后设置softnetirq标记等待处理函数来处理。通过上面的NAPI的流程学习我们知道这里的处理函数net_rx_action会遍历poll_list，然后一次调用遍历得到的net_device的poll方法。于是我们的net_device_2的poll方法被调用。此时这个poll方法就是由内核实现的了，一般是绑定到内核提供的process_backlog函数上。这个函数做的事情就是从softnet_data的nput_pkt_queue链表中获取一个sk_buff，然后调用上面讲到的netif_receive_skb让这个数据包开始走协议栈。可以看到通过修改netif_rx的实现对于老的驱动代码不需要改动就能共用同一套NAPI的基础架构。

那么NAPI有什么好处呢？对于Neutron网络节点来说，在非DVR这种情况下列如外网口的这类网卡，其数据包的个数经常是很大的，数据包量大了后中断的方式效率就太低下了，所以通过NAPI的poll方法可以减少网卡的负担，由CPU主动根据情况去网卡上获取数据包。另外NAPI其实真实的实现是会在中断和轮询这两者间切换的，比如一个新的数据包到达，然后发现内核很繁忙且还有未处理的数据包的时候，其会将中断模式改为poll，于是新的数据包到达后就不会打断内核，同时当内核空闲的时候其会去轮询设备，查看是否有要处理的数据包。当压力下来后又可以恢复中断模式。通过这种方法在有大量的数据包环境中可以减少CPU使用率，并在只有少量数据包的时候降低响应时间。

另外这里希望大家能想一下从网卡收到包到这个包触发硬件中断以及等待softirq的执行直到最终准备走协议栈，这一切是在哪个CPU核上发生的？这些对性能是否有影响？

现在我们的数据包sk_buff已经站在内核协议栈的门口了，通过netif_receive_skb其会开始内核协议栈之旅，对于netif_receive_skb在我们后面的章节中会讲到其实现。



### 发包路径

我们来看发包路径。应用层的某个应用调用socket发包后，数据包会组织成sk_buff结构体，然后经过协议栈一直往下直到协议栈的最下层开始准备发包。此时内核会调用net_device的某个函数将sk_buff发送出去。我们来看下这个流程。

在看流程之前我们要先看下TC(Traffic Control)。在本书的前面章节已经介绍了tc命令的用法以及qdisc的概念，这里再来复习一下。内核中的发包可以简化的看成这么一个过程：内核将要发送的sk_buff放到一个队列中，然后内核再从这个队列中取出一个数据包，然后再调用net_device的驱动实现的发包函数将数据包传到网卡上让网卡把包发送出去，伪代码如下：

```
queue.put(sk_buff)
sk_buff = queue.get()
net_device.hardware_send_packet(sk_buff)
```

可能有人要问了：为什么不直接调用net_device.hardware_send_packet将sk_buff发送出去而要走一下队列呢？原因在于我们要做TC，也就是要做流量的整形，限速就是流量整形的一种，比如我希望控制发包速率在每秒100个，否则我的交换机等物理设备承受不了。上层应用当然不会去主动做这个限速啦，所以这个事情得内核在做。怎么做呢？就是通过这个队列。比如我要限制每秒100个包，那么内核只要实现一个队列，这个队列的queue.get()方法每10ms才会返回一个数据包（即便很多数据包都积压在队列中了），然后在不考虑hardware_send_packet等函数的开销的时候我们就将发包的速率控制在了100pps了。

这里的queue在net_device中就是qdisc属性。我们可以用tc配置各种规则决定如何对流量进行整形（这就是tc的实现了，有兴趣的同学可以学习下）。所以我们可以看到内核在发包的时候是可以做限速的。在我们Neutron提供服务的时候，QoS是很重要的一块，如果大家希望实现限速那么可以考虑使用tc。

现在来看下完整的流程，上面的流程没有考虑太多的异常情况，其中最重要的一种异常是网卡繁忙。什么是网卡繁忙呢？我们知道系统中一般只有两到四块网卡，但是却往往有30几个CPU核。假设同一时间这30几个核都调用了发包流程，那么我们的网卡是不能同时响应的。在不考虑多队列等情况下，如果我有4块网卡，那么同一时间只能同时满足4个核的需求，此时其它核的发包请求就失败了。也就是我们上面代码中的net_device.hardware_send_packet(sk_buff)会返回错误。此时总不能把这个数据包丢掉吧，这样重传的代价太大了，那么咋办呢？此时我们的发包的softirq就开始起作用了。

当net_device.hardware_send_packet(sk_buff)无法正常工作的时候（其实内核通过锁机制并不会真的去调用该方法），内核会执行netif_schedule这个内核提供的函数将这个net_device放到每个CPU核的softnet_data的output_queue中，然后标记下NET_TX_SOFTIRQ这个标记位，接着会将这个没有发送出去的sk_buff重新调用上面伪代码中说的queue.put(sk_buff)放回到qdisc的队列中。然后此次发送就认为是成功了，内核会返回到上层调用代码出。

接着softirq开始执行，NET_TX_SOFTIRQ对应的处理函数会遍历每个CPU核的softnet_data的output_queue，对上面的net_device调用sk_buff = queue.get()以及net_device.hardware_send_packet(sk_buff)这个流程，如果发送成功那就太好了，如果还是失败的话就继续走上面的流程，把net_device继续留在ouput_queue中，把sk_buff继续放回到队列里，然后再标记下NET_TX_SOFTIRQ等着下一次继续处理。

对于qdisc来说，其队列提供的方法比我们伪代码提供的要复杂一些。主要有三个：
1. enqueue：类似我们put，第一次放一个sk_buff到qdisc中
2. dequeue：类似我们的get，从qdisc重取出一个sk_buff
3. requeue：类似我们发送失败后执行的put，将sk_buff再次放回qdisc

大家如果深入的去学习过tc的话，对于这三个方法肯定印象会比较深刻，记得小秦一开始学tc的时候觉得有些概念特别难以理解，现在知道了发包时候走tc的流程后就觉得简单了许多。

另外要提一下就是不是所有的设备都会的走qdisc这一套流程，有可能直接调用发包函数发包出去了。比如对于lo这种设备就没有qdisc的相关操作。这一点我们从下面的代码里可以看到：

```
......
    if (q->enqueue) {
        rc = __dev_xmit_skb(skb, q, dev, txq);
        goto out; 
    }    

    /* The device has no queue. Common case for software devices:
       loopback, all the sorts of tunnels...

       Really, it is unlikely that netif_tx_lock protection is necessary
       here.  (f.e. loopback and IP tunnels are clean ignoring statistics
       counters.)
       However, it is possible, that they rely on protection
       made by us here.

       Check this and shot the lock. It is not prone from deadlocks.
       Either shot noqueue qdisc, it is even simpler 8)
     */
......
static const struct net_device_ops loopback_ops = {
    .ndo_init      = loopback_dev_init,
    .ndo_start_xmit= loopback_xmit,
    .ndo_get_stats64 = loopback_get_stats64,
    .ndo_set_mac_address = eth_mac_addr,
};
```

这里还是要提个问题给大家，当我应用调用socket发包的时候，这个数据包的发送过程是都在一个CPU核上发生的吗？

现在我们对于一个数据包怎么从协议栈的底部到出网卡已经大致清楚了。如果大家平时发现NET_TX_SOFTIRQ这个softirq执行的频率很高(此时根据我们之前说的ksoftirqd的CPU占用率也会很高)，那么可能是网卡过于繁忙造成的。


### lo/veth的实现


这里我们来看两个虚拟设备的驱动收发包过程。我们上面讲的例子都是以物理网卡来讲的，但实际上在内核中物理网卡和虚拟网卡都是通过net_device来保存期相关统计信息、操作函数等信息的。我们会分析两个设备的实现，一个是loopback设备，另一个是veth。lo大家肯定都不陌生，veth的话我们在Neutron中已经看到过了，其会用于连接br-int以及我们的qbr。

首先先来看lo的实现。我们看下一个数据包从lo口发送的时候会发生什么吧。根据我们前一节讲的，当一个包被发送的时候，会的准备走net_device的qdisc队列进行流量的整形。但是我们也讲了如果net_device的qdisc的enqueue方法为空的话则不会走这段逻辑。我们的loopback设备的net_device就没有这个enqueue方法，所以其会的直接调用net_device上的发送函数（也就是ndo_start_xmit这个函数指针）。对于lo设备来说这个函数的实现为loopback_xmit。我们看下其实现：

```
static netdev_tx_t loopback_xmit(struct sk_buff *skb,
                 struct net_device *dev)
{
    struct pcpu_lstats *lb_stats;
    int len;

    skb_orphan(skb);

    /* Before queueing this packet to netif_rx(),
     * make sure dst is refcounted.
     */
    skb_dst_force(skb);

    skb->protocol = eth_type_trans(skb, dev);

    /* it's OK to use per_cpu_ptr() because BHs are off */
    lb_stats = this_cpu_ptr(dev->lstats);

    len = skb->len;
    if (likely(netif_rx(skb) == NET_RX_SUCCESS)) {
        u64_stats_update_begin(&lb_stats->syncp);
        lb_stats->bytes += len;
        lb_stats->packets++;
        u64_stats_update_end(&lb_stats->syncp);
    }

    return NETDEV_TX_OK;
}
```

我们上面说过，物理设备的发包在这里应该是调用驱动相关的代码发送数据包出去，但是在loopback_xmit中我们看到其并没有真实的发送数据包出去，而是直接调用了netif_rx（还记的netif_rx不？不记得的话可以翻翻上面收包的小节哈）。netif_rx会把我们的数据包sk_buff放到cpu核的softnet_data的input_queue上，然后就开始走正常的收包流程了。

接着我们再来看下veth的实现。veth的相关ops实现为：

```
static const struct net_device_ops veth_netdev_ops = { 
    .ndo_init            = veth_dev_init,
    .ndo_open            = veth_open,
    .ndo_stop            = veth_close,
    .ndo_start_xmit      = veth_xmit,
    .ndo_change_mtu      = veth_change_mtu,
    .ndo_get_stats64     = veth_get_stats64,
    .ndo_set_rx_mode     = veth_set_multicast_list,
    .ndo_set_mac_address = eth_mac_addr,
#ifdef CONFIG_NET_POLL_CONTROLLER
    .ndo_poll_controller    = veth_poll_controller,
#endif
    .ndo_get_iflink     = veth_get_iflink,
};
```

所以我们看下veth_xmit的实现。现在我们的数据包准备从veth的某个端点发送出去了，然后sk_buff传到了veth_xmit，其实现为：

```
static netdev_tx_t veth_xmit(struct sk_buff *skb, struct net_device *dev)
{
    struct veth_priv *priv = netdev_priv(dev);
    struct net_device *rcv;
    int length = skb->len;

    rcu_read_lock();
    rcv = rcu_dereference(priv->peer);
    if (unlikely(!rcv)) {
        kfree_skb(skb);
        goto drop;
    }   
    /* don't change ip_summed == CHECKSUM_PARTIAL, as that
     * will cause bad checksum on forwarded packets
     */
    if (skb->ip_summed == CHECKSUM_NONE &&
        rcv->features & NETIF_F_RXCSUM)
        skb->ip_summed = CHECKSUM_UNNECESSARY;

    if (likely(dev_forward_skb(rcv, skb) == NET_RX_SUCCESS)) {
        struct pcpu_vstats *stats = this_cpu_ptr(dev->vstats);

        u64_stats_update_begin(&stats->syncp);
        stats->bytes += length;
        stats->packets++;
        u64_stats_update_end(&stats->syncp);
    } else {
drop:
        atomic64_inc(&priv->dropped);
    }
    rcu_read_unlock();
    return NETDEV_TX_OK;
}
```

这里一般都会走dev_forward_skb函数。dev_forward_skb是内核提供的一个函数，我们在下面章节中讲bridge的时候会看到其主要功能。这里其用处是用于将某个数据包直接放某个CPU的softnet_data的input_queue上，并设置某个网卡来处理这个数据包（也就是内核直接让某个网卡来处理一个内存中的sk_buff包。可以简单的认为让某个网卡处理这个包要做的事情就是把这个网卡的net_device放到softnet_data的poll_list上然后将sk_buff的net_device的指针指向这个网卡的net_device）。于是这个被选中的网卡就会来处理这个包了。那么对于veth来说是哪个网卡被选中了做这个事情呢？从struct veth_priv *priv = netdev_priv(dev)和rcv = rcu_dereference(priv->peer)这两行代码就能知道这里是从veth的net_device的私有数据中找到对应的peer网卡，然后找到的peer网卡就是我们选中的目的网卡了。


### 多队列/RSS/RPS/RFS

在本章我们讲过了一个数据包的收包、发包路径。在相应的内容中笔者也提出过一个疑问给大家：一个数据包的收、发的过程是发生在哪个核上的？其实如果大家按照本章之前的内容来分析的话，会发现一个网卡的数据包基本上都是在一个核上进行收发的。比方说一个数据包到达了网卡，网卡触发中断通知某个CPU，然后接下来的事情就基本上都是这个CPU来处理了。我们知道对于一个物理机来说CPU的核数可能能有二三十个，但网卡的个数一般只有四块。那按照这个逻辑一个物理机上最多也就四个CPU能处理网络的包咯？对于Neutron的网络节点来说这个是不能接受的，因为这会让CPU严重成为性能提高的瓶颈。所以在这一节我们来讲下一个最常见的针对这个问题的优化方法。

这个方法的核心思想很简单：上面说的瓶颈之所以产生的原因我们知道是因为一个网卡对应一个中断向量号，而一个中断只能被一个CPU响应。那么能不能让这个网卡产生的中断分散到各个CPU上呢？一种方法就是多队列。这里的多队列指的是网卡本身硬件支持多队列，每个队列我们可以简单的看成就是一个能起到收发包作用的网卡，一个数据包到达网卡时，根据某个hash算法这个数据包会放到某个队列上，然后这个队列负责发送一个中断给CPU。假设一个网卡有24个队列，那么就可以绑定最多24个CPU，这样就能充分发挥CPU的使用率了。

我们来看下这种多队列的配置方法。内核开发者Robert Olsson写了一个脚本帮助大家简单的配置多队列，大家可以直接使用这个脚本进行配置：

```
# setting up irq affinity according to /proc/interrupts  
# 2008-11-25 Robert Olsson  
# 2009-02-19 updated by Jesse Brandeburg  
#  
# > Dave Miller:  
# (To get consistent naming in /proc/interrups)  
# I would suggest that people use something like:  
#       char buf[IFNAMSIZ+6];  
#  
#       sprintf(buf, "%s-%s-%d",  
#               netdev->name,  
#               (RX_INTERRUPT ? "rx" : "tx"),  
#               queue->index);  
#  
#  Assuming a device with two RX and TX queues.  
#  This script will assign:   
#  
#       eth0-rx-0  CPU0  
#       eth0-rx-1  CPU1  
#       eth0-tx-0  CPU0  
#       eth0-tx-1  CPU1  
#  
  
set_affinity()  
{  
    MASK=$((1<<$VEC))  
    printf "%s mask=%X for /proc/irq/%d/smp_affinity\n" $DEV $MASK $IRQ  
    printf "%X" $MASK > /proc/irq/$IRQ/smp_affinity  
    #echo $DEV mask=$MASK for /proc/irq/$IRQ/smp_affinity  
    #echo $MASK > /proc/irq/$IRQ/smp_affinity  
}  
  
if [ "$1" = "" ] ; then  
        echo "Description:"  
        echo "    This script attempts to bind each queue of a multi-queue NIC"  
        echo "    to the same numbered core, ie tx0|rx0 --> cpu0, tx1|rx1 --> cpu1"  
        echo "usage:"  
        echo "    $0 eth0 [eth1 eth2 eth3]"  
fi  
  
  
# check for irqbalance running  
IRQBALANCE_ON=`ps ax | grep -v grep | grep -q irqbalance; echo $?`  
if [ "$IRQBALANCE_ON" == "0" ] ; then  
        echo " WARNING: irqbalance is running and will"  
        echo "          likely override this script's affinitization."  
        echo "          Please stop the irqbalance service and/or execute"  
        echo "          'killall irqbalance'"  
fi  
  
#  
# Set up the desired devices.  
#  
  
for DEV in $*  
do  
  for DIR in rx tx TxRx  
  do  
     MAX=`grep $DEV-$DIR /proc/interrupts | wc -l`  
     if [ "$MAX" == "0" ] ; then  
       MAX=`egrep -i "$DEV:.*$DIR" /proc/interrupts | wc -l`  
     fi  
     if [ "$MAX" == "0" ] ; then  
       echo no $DIR vectors found on $DEV  
       continue  
       #exit 1  
     fi  
     for VEC in `seq 0 1 $MAX`  
     do  
        IRQ=`cat /proc/interrupts | grep -i $DEV-$DIR-$VEC"$"  | cut  -d:  -f1 | sed "s/ //g"`  
        if [ -n  "$IRQ" ]; then  
          set_affinity  
        else  
           IRQ=`cat /proc/interrupts | egrep -i $DEV:v$VEC-$DIR"$"  | cut  -d:  -f1 | sed "s/ //g"`  
           if [ -n  "$IRQ" ]; then  
             set_affinity  
           fi  
        fi  
     done  
  done  
done  
```

我们来看下关键的代码。关键的代码是：
```
IRQ=`cat /proc/interrupts | grep -i $DEV-$DIR-$VEC"$"  | cut  -d:  -f1 | sed "s/ //g"`  
```
以及：
```
printf "%X" $MASK > /proc/irq/$IRQ/smp_affinity  
```
第一个关键代码的目的是获取网卡的某个队列其绑定的中断向量号，此时由于没有做别的什么操作，所以任何一个队列产生的数据包都只会发送到开机启动的时候系统所默认的CPU上，一般也就是CPU0。因此此时虽然有多队列但是所有队列依旧只和CPU0相关，这里我们要做的就是将某个队列从CPU0绑定到其他CPU上。这个队列可以通过其IRQ识别出，然后这个IRQ通过向/proc/irq/$IRQ/smp_affinity 写入一个掩码来将其和某个CPU绑定。掩码类似00000001这种，每一位表示一个CPU。比如00000001就表示CPU0。

RSS(Receive Side Scaling)，指的就是上面这种通过硬件多队列来提升CPU使用率进而优化性能的一种方法。

笔者所接触过的一些托管云用户有时候会使用一些老机器来做网络节点，这些机器的网卡很老，不支持硬件多队列，此时该如何优化呢？这就得用上RPS(Receive Packet Steering)了。什么是RPS呢？其实很简单，RPS就是软件实现的一种RSS。本来一个数据包到达网卡后走哪个队列是硬件决定的，现在由于硬件没有多队列所以硬件中断都是发生在一个CPU上。但是通过本书前面的学习我们知道真正的开销其实是softirq这个下半区中进行的，如果此时什么都不做的话，所有的开销都会发生在这个响应硬件中断的CPU上，此时这个CPU(一般是CPU0)就会不堪重负。但是通过RPS技术，硬件中断响应后内核会将这个sk_buff平衡的放到别的CPU的softnet_data的input_queue上，并且设置相应的softirq的标记，此时最耗时的softirq的工作就能由别的CPU负担了。这就是RPS技术的基本思想。

最后我们来看个RFS(Receive Flow Steering)。我们都知道内存在计算机中是一种金字塔结构，CPU的cache最快，然后是内存，然后是磁盘。如果一个数据流都能在一个CPU上进行处理那么就能充分利用cache。但是按照我们上面讲的RSS或者RPS，一个数据包走哪个CPU核是由一个hash算法决定的，这个hash算法一般只会通过源、目的IP或者mac来判断走哪个CPU。所以这个hash算法是看不到数据流(flow)层面的东西的。RFS可以看成是一个使用了高级点的hash算法的RSS，其保证同一个数据流的包都走同一个CPU。

最后如果大家对这些多队列的技术感兴趣的话可以看下https://www.kernel.org/doc/Documentation/networking/scaling.txt。链接中的文档包含了这些技术的说明、使用方法及相关原理。


### LSO/LRO/GSO/GRO/TSO/USO

首先先来讲一个术语：offload。这个在本书之前的内容中也提过，比如offload vxlan等。什么叫offload呢？很简单，就是将一个本来由软件实现的功能现在放到硬件上来实现。这里的offload vxlan的意思就是本来我vxlan的封包解包是由ovs来做的，offload后就由网卡或交换机帮我做了。

LSO/LRO/GSO/GRO/TSO/USO等其实就是一种offload技术。先来说下全称：

* LSO：Large Segment Offload
* LRO：Large Receive Offload
* GSO：Generic Segmentation Offload
* GRO：Generic Receive Offload
* TSO：TCP Segmentation Offload
* USO：UDP Fragmentation offload

我们知道当数据包在传输的时候按照标准是必须分割成一个一个小于MTU的小包进行传输的，然后传输到另一头后另一头还得将这些数据包拼装回去。这些分割、拼装的事情都是软件实现的，于是就有人想将这些事情offload到硬件上去实现，接着就产生了上面的这些技术。

* LSO：协议栈直接传递打包给网卡，由网卡负责分割
* LRO：网卡对零散的小包进行拼装，返回给协议栈一个大包
* GSO：LSO需要用户区分网卡是否支持该功能，GSO则会自动判断，如果支持则启用LSO，否则不启用
* GRO：LRO需要用户区分网卡是否支持该功能，GRO则会自动判断，如果支持则启用LRO，否则不启用
* TSO：针对TCP的分片的offload。类似LSO、GSO，但这里明确是针对TCP
* USO：正对UDP的offload，一般是IP层面的分片处理

实际大家在使用中，必须做好充分的测试。


### DPDK

本章中说的收发包走的都是Linux内核提供的一套流程。而DPDK则不是。DPDK是一套Intel委托6wind开发的开源的数据收发库。当一个数据包进入网卡产生中断后，响应这个中断的驱动是DPDK安装的驱动。这个驱动会通过UIO机制直接让用户态可以直接操作这个数据包。在用户态用户可以写一个程序通过DPDK提供的API处理这个数据包，比如直接在用户态写一个二层转发实现，或者在用户态直接实现一个vRouter等。用户态的好处是其可以充分利用CPU。内核态的协议栈存在一个问题：处理包的性能和CPU核数不成正比。当CPU的核数增加到一定个数后，再增加CPU核数对于数据包的收发作用就不大了。因此大的公司可以基于DPDK直接在用户态实现一个全新的协议栈程序，只实现自己需要的功能，显而易见这个协议栈程序肯定会是一个多线程的程序用于充分挖掘多核CPU的潜力。

对于存在性能瓶颈且使用了很多方法优化都没法提高性能但又不希望使用硬件方案的用户来说，使用基于DPDK或类似技术实现的协议栈是一个可以考虑的选择。


